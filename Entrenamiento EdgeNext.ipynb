{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d28258c-31d4-4ac5-88bc-5103e2a9863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epsilon\\anaconda3\\envs\\explainable_ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score, confusion_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from isic_class import ISICDataset\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5470dc0c-ccd0-4c13-b205-dccb0853c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths y nombres\n",
    "DATA_PATH = \"winner_data/\"\n",
    "METADATA_PATH = os.path.join(DATA_PATH, \"metadata/\")\n",
    "\n",
    "IMAGES_PATH = os.path.join(DATA_PATH, \"images/\")\n",
    "SYNTHETIC_DATA_PATH = os.path.join(\"winner_data/\")\n",
    "\n",
    "ORIGINAL_IMAGES_PATH = os.path.join(IMAGES_PATH, \"original/\")\n",
    "SYNTHETIC_IMAGES_PATH = os.path.join(IMAGES_PATH, \"synthetic/\")\n",
    "\n",
    "# Nombres csv con datos\n",
    "TRAIN_ORIGINAL_DF_FILENAME = \"train_original.csv\"\n",
    "VAL_DF_FILENAME = \"val.csv\"\n",
    "TEST_DF_FILENAME = \"test.csv\"\n",
    "SYNTHETIC_DF_FILENAME = \"synthetic.csv\"\n",
    "\n",
    "TRAIN_ORIGINAL_DF_PATH = os.path.join(METADATA_PATH, TRAIN_ORIGINAL_DF_FILENAME)\n",
    "VAL_DF_PATH = os.path.join(METADATA_PATH, VAL_DF_FILENAME)\n",
    "SYNTHETIC_DF_PATH = os.path.join(METADATA_PATH, SYNTHETIC_DF_FILENAME)\n",
    "\n",
    "# Modelos\n",
    "MODELS_PATH = \"models/\"\n",
    "EXPERIMENT_NAME = \"edgenext_last_layer/\"\n",
    "MODEL_SAVE_PATH = os.path.join(MODELS_PATH, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb844af8-0336-4f2a-afbc-7171bfd6805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propiedades data\n",
    "IMAGE_ID_COL = \"isic_id\"\n",
    "IMAGE_PATH_COL = \"image_path\"\n",
    "TARGET_COL = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff559df4-9d86-4672-9a44-e548117390c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros modelo\n",
    "MODEL_NAME = 'edgenext_base.in21k_ft_in1k'\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26c59353-0bca-4650-b2b6-f6f87e1fd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHS = 30\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "WEIGHT_DECAY = 0.01  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "872d60df-8803-44e1-977d-24006dc9e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "# Configurar GPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56536f38-47d0-4cce-86d7-45497adf7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilos Dataloader\n",
    "if os.cpu_count() > 14:\n",
    "    NUM_WORKERS = 14\n",
    "else:\n",
    "    NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db8f99-7c97-49ac-a3d3-4e3973cedf96",
   "metadata": {},
   "source": [
    "## 2. Transformaciones imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52312e8a-0218-41ad-9d75-291dcf5ee45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener data de entrenamiento modelo preentrenado HuggingFace\n",
    "model_cfg = timm.get_pretrained_cfg(MODEL_NAME)\n",
    "IMG_SIZE = model_cfg.input_size[1]\n",
    "NORM_MEAN = model_cfg.mean\n",
    "NORM_STD = model_cfg.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d71fd95c-02b1-44c1-8a45-8d836871c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para evitar overfitting en train\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c7a51-a41a-4ebe-9fa2-1b3785dbc709",
   "metadata": {},
   "source": [
    "## 3. Cargar datasets y dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bddd2c3-8139-4169-baf2-f83de296568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataframes\n",
    "train_original_df = pd.read_csv(TRAIN_ORIGINAL_DF_PATH)\n",
    "val_df = pd.read_csv(VAL_DF_PATH)\n",
    "synthetic_df = pd.read_csv(SYNTHETIC_DF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e15d5d8e-b8a2-4d59-a2ed-0389335ca13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_4257482</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_4257482.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_8604888</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_8604888.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0209050</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_0209050.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_8099764</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_8099764.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_6615693</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_6615693.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  target                                    image_path\n",
       "0  ISIC_4257482       0  winner_data/images/original/ISIC_4257482.jpg\n",
       "1  ISIC_8604888       0  winner_data/images/original/ISIC_8604888.jpg\n",
       "2  ISIC_0209050       0  winner_data/images/original/ISIC_0209050.jpg\n",
       "3  ISIC_8099764       0  winner_data/images/original/ISIC_8099764.jpg\n",
       "4  ISIC_6615693       0  winner_data/images/original/ISIC_6615693.jpg"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "762498f3-a9ef-4919-8921-5379a135e7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_4260773</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_4260773.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_9697201</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_9697201.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_8301065</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_8301065.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_3753800</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_3753800.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0690097</td>\n",
       "      <td>0</td>\n",
       "      <td>winner_data/images/original/ISIC_0690097.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  target                                    image_path\n",
       "0  ISIC_4260773       0  winner_data/images/original/ISIC_4260773.jpg\n",
       "1  ISIC_9697201       0  winner_data/images/original/ISIC_9697201.jpg\n",
       "2  ISIC_8301065       0  winner_data/images/original/ISIC_8301065.jpg\n",
       "3  ISIC_3753800       0  winner_data/images/original/ISIC_3753800.jpg\n",
       "4  ISIC_0690097       0  winner_data/images/original/ISIC_0690097.jpg"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2d464ef-0797-4f84-9f49-1ba6cdb53822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>winner_data/images/synthetic/0/lr\\00047c6d-ca1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>winner_data/images/synthetic/0/lr\\00808b91-e14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>winner_data/images/synthetic/0/lr\\008dcaf9-71b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>winner_data/images/synthetic/0/lr\\009d9c71-b0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>winner_data/images/synthetic/0/lr\\00b846be-652...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                         image_path\n",
       "0       1  winner_data/images/synthetic/0/lr\\00047c6d-ca1...\n",
       "1       1  winner_data/images/synthetic/0/lr\\00808b91-e14...\n",
       "2       1  winner_data/images/synthetic/0/lr\\008dcaf9-71b...\n",
       "3       1  winner_data/images/synthetic/0/lr\\009d9c71-b0d...\n",
       "4       1  winner_data/images/synthetic/0/lr\\00b846be-652..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884a756-d12b-4cbe-9982-cad2bf16aa38",
   "metadata": {},
   "source": [
    "## 4. Metrica custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bab20e5-1526-4660-af92-071daef9ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrica competencia\n",
    "def custom_metric(estimator, X, y_true):\n",
    "    y_hat = estimator.predict_proba(X)[:, 1]\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "    \n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    \n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e00151-37f6-4697-bd2f-2f0554665cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_custom_partial_auc(y_true, y_pred_proba_positive_class):\n",
    "    min_tpr_threshold = 0.80\n",
    "    max_fpr_threshold = abs(1 - min_tpr_threshold)\n",
    "\n",
    "    v_gt = np.abs(y_true - 1) \n",
    "    v_pred_proba_negative_class = 1.0 - y_pred_proba_positive_class\n",
    "    \n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred_proba_negative_class, max_fpr=max_fpr_threshold)\n",
    "    custom_scaled_auc = partial_auc_scaled\n",
    "    true_partial_auc = custom_scaled_auc * (max_fpr_threshold - 0.5 * max_fpr_threshold**2) + 0.5 * max_fpr_threshold**2\n",
    "    \n",
    "    return true_partial_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb21b1-af34-4eb8-bdfa-0d65d3e742b1",
   "metadata": {},
   "source": [
    "## 4. Funciones para entrenamiento y metricas por epoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a94051b-3c33-4655-9154-426fffab8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch_num, num_epochs, num_classes):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_labels_list = []\n",
    "    all_preds_proba_list = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Guardar metricas y labels de epoca\n",
    "        all_labels_list.extend(labels.detach().cpu().numpy())\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()\n",
    "        all_preds_proba_list.extend(probs)\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"  Epoch [{epoch_num+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Convertir lista a numpy array para sklearn\n",
    "    all_labels_np = np.array(all_labels_list)\n",
    "    all_preds_proba_np = np.array(all_preds_proba_list)\n",
    "    \n",
    "    # Calculo de metricas para la epoca\n",
    "    predicted_classes_np = (all_preds_proba_np >= 0.5).astype(int)\n",
    "    epoch_auc = roc_auc_score(all_labels_np, all_preds_proba_np)\n",
    "    epoch_f1 = f1_score(all_labels_np, predicted_classes_np, pos_label=1, zero_division=0)\n",
    "    epoch_recall = recall_score(all_labels_np, predicted_classes_np, pos_label=1, zero_division=0)\n",
    "    epoch_precision = precision_score(all_labels_np, predicted_classes_np, pos_label=1, zero_division=0)\n",
    "    epoch_custom_partial_auc = calculate_custom_partial_auc(all_labels_np, all_preds_proba_np)\n",
    "    epoch_balanced_acc = balanced_accuracy_score(all_labels_np, predicted_classes_np)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Epoch [{epoch_num+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, AUC: {epoch_auc:.4f}, PrAUC : {epoch_custom_partial_auc:.4f}, BalAcc: {epoch_balanced_acc:.4f}, F1: {epoch_f1:.4f}, Recall: {epoch_recall:.4f}, Precision: {epoch_precision:.4f}, Time: {epoch_duration:.2f}s\")\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'auc': epoch_auc,\n",
    "        'partial_auc': epoch_custom_partial_auc,\n",
    "        'balanced_accuracy': epoch_balanced_acc,\n",
    "        'f1_score': epoch_f1,\n",
    "        'recall': epoch_recall,\n",
    "        'precision': epoch_precision\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e69cf755-5055-4caf-a39d-34c31156263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device, epoch_num, num_epochs, num_classes):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_labels_list = []\n",
    "    all_preds_proba_list = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            all_labels_list.extend(labels.detach().cpu().numpy())\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()\n",
    "            all_preds_proba_list.extend(probs)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    \n",
    "    all_labels_np = np.array(all_labels_list)\n",
    "    all_preds_proba_np = np.array(all_preds_proba_list)\n",
    "\n",
    "    if len(np.unique(all_labels_np)) < 2 :\n",
    "        print(f\"Warning: Conjunto de validacion para epoca {epoch_num+1} le falta una clase!.\")\n",
    "        epoch_auc = 0.5\n",
    "    else:\n",
    "        epoch_auc = roc_auc_score(all_labels_np, all_preds_proba_np)\n",
    "    \n",
    "    predicted_classes_np = (all_preds_proba_np >= 0.5).astype(int)\n",
    "    epoch_f1 = f1_score(all_labels_np, predicted_classes_np, pos_label=1, zero_division=0)\n",
    "    epoch_recall = recall_score(all_labels_np, predicted_classes_np, pos_label=1, zero_division=0)\n",
    "    epoch_precision = precision_score(all_labels_np, predicted_classes_np, pos_label=1, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(all_labels_np, predicted_classes_np, labels=[0,1])\n",
    "    epoch_custom_partial_auc = calculate_custom_partial_auc(all_labels_np, all_preds_proba_np)\n",
    "    epoch_balanced_acc = balanced_accuracy_score(all_labels_np, predicted_classes_np)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f\"Epoch [{epoch_num+1}/{num_epochs}] Val Loss: {epoch_loss:.4f}, AUC: {epoch_auc:.4f}, PrAUC: {epoch_custom_partial_auc:4f}, BalAcc: {epoch_balanced_acc:.4f}, F1(pos): {epoch_f1:.4f}, Recall(pos): {epoch_recall:.4f}, Precision(pos): {epoch_precision:.4f}, Time: {epoch_duration:.2f}s\")\n",
    "    if conf_matrix is not None:\n",
    "        print(f\"Matriz de confusion para epoca {epoch_num+1}:\\n{conf_matrix}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'auc': epoch_auc,\n",
    "        'partial_auc': epoch_custom_partial_auc,\n",
    "        'balanced_accuracy': epoch_balanced_acc,\n",
    "        'f1_score': epoch_f1,\n",
    "        'recall': epoch_recall,\n",
    "        'precision': epoch_precision,\n",
    "        'conf_matrix': conf_matrix\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfe50864-9cca-4c32-a2b1-a5881ecfd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para contener entrenamiento debido a NUM_WORKERS > 0\n",
    "def start_training():\n",
    "    # Combinar imagenes sinteticas y originales\n",
    "    required_cols = [IMAGE_PATH_COL, TARGET_COL]\n",
    "    train_df_combined = pd.concat(\n",
    "        [train_original_df[required_cols], synthetic_df[required_cols]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Crear dataset entrenamiento\n",
    "    train_dataset = ISICDataset(\n",
    "        dataframe=train_df_combined,\n",
    "        image_path_col=IMAGE_PATH_COL,\n",
    "        target_col=TARGET_COL,\n",
    "        transforms=train_transforms\n",
    "    )\n",
    "    \n",
    "    # Crear dataset validacion\n",
    "    val_dataset = ISICDataset(\n",
    "        dataframe=val_df,\n",
    "        image_path_col=IMAGE_PATH_COL,\n",
    "        target_col=TARGET_COL,\n",
    "        transforms=val_transforms\n",
    "    )\n",
    "\n",
    "    # Cargar modelo preentrenado\n",
    "    model = timm.create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=NUM_CLASSES\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    \"\"\"\n",
    "    # Descongelar capas\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \"\"\"\n",
    "    # Congelar capas\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Descongelar ultima capa\n",
    "    for param in model.head.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Pesos para clases en loss y randomsampler\n",
    "    class_counts = train_df_combined[TARGET_COL].value_counts().sort_index()\n",
    "    weight_class_0 = 1.0 / (class_counts.get(0, 1e-9))\n",
    "    weight_class_1 = 1.0 / (class_counts.get(1, 1e-9))\n",
    "    weight_sum = weight_class_0 + weight_class_1\n",
    "\n",
    "    # Funcion loss\n",
    "    loss_weight_class_0 = weight_class_0 / weight_sum\n",
    "    loss_weight_class_1 = weight_class_1 / weight_sum\n",
    "    loss_weights = torch.tensor([loss_weight_class_0, loss_weight_class_1], dtype=torch.float).to(DEVICE)\n",
    "    criterion =  nn.CrossEntropyLoss(weight=loss_weights)\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    # Pesos para contrarrestar desbalance. Hacemos oversampling\n",
    "    print(f\"--- Creando WeightedRandomSampler ---\")\n",
    "    weights_train_list = [weight_class_0 if t == 0 else weight_class_1 for t in train_df_combined[TARGET_COL]]\n",
    "    weights_train = torch.DoubleTensor(weights_train_list)\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=weights_train,\n",
    "        num_samples=len(weights_train),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # Dataloaders\n",
    "    print(f\"--- Creando DataLoaders ---\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True if DEVICE.type == 'cuda' else False, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True if DEVICE.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    # Loop entrenamiento\n",
    "    best_val_auc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_auc': [],\n",
    "        'train_pr_auc': [],\n",
    "        'train_bal_acc': [],\n",
    "        'train_f1': [],\n",
    "        'train_precision': [],\n",
    "        'train_recall': [],\n",
    "        'val_loss': [],\n",
    "        'val_auc': [],\n",
    "        'val_pr_auc': [],\n",
    "        'val_bal_acc': [],\n",
    "        'val_f1': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "    }\n",
    "    BEST_MODEL_FULL_PATH = os.path.join(MODEL_SAVE_PATH, \"edgenext_best.pth\")\n",
    "\n",
    "    print(f\"\\n --- Comenzado loop entrenamiento por {MAX_EPOCHS} epochs ---\")\n",
    "    print(f\"Mejor modelo se guardara como: {BEST_MODEL_FULL_PATH}\")\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{MAX_EPOCHS} =====\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\")\n",
    "\n",
    "        # Guardado de metricas\n",
    "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE, epoch, MAX_EPOCHS, NUM_CLASSES)\n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['train_auc'].append(train_metrics['auc'])\n",
    "        history['train_pr_auc'].append(train_metrics['partial_auc'])\n",
    "        history['train_bal_acc'].append(train_metrics['balanced_accuracy'])\n",
    "        history['train_f1'].append(train_metrics['f1_score'])\n",
    "        history['train_precision'].append(train_metrics['precision'])\n",
    "        history['train_recall'].append(train_metrics['recall'])\n",
    "\n",
    "        val_metrics = validate_one_epoch(model, val_loader, criterion, DEVICE, epoch, MAX_EPOCHS, NUM_CLASSES)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_auc'].append(val_metrics['auc'])\n",
    "        history['val_pr_auc'].append(val_metrics['partial_auc'])\n",
    "        history['val_bal_acc'].append(val_metrics['balanced_accuracy'])\n",
    "        history['val_f1'].append(val_metrics['f1_score'])\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        \n",
    "        current_val_auc = val_metrics['partial_auc']\n",
    "        if current_val_auc > best_val_auc:\n",
    "            print(f\"Val PrAUC mejoro ({best_val_auc:.4f} --> {current_val_auc:.4f}). Guardando modelo\")\n",
    "            best_val_auc = current_val_auc\n",
    "            torch.save(model.state_dict(), BEST_MODEL_FULL_PATH)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Val AUC ({current_val_auc:.4f}) no mejoro desde la mejor ({best_val_auc:.4f}). Ninguna mejora durante {epochs_no_improve} epochs.\")\n",
    "\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping {EARLY_STOPPING_PATIENCE} epochs sin mejora\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Duracion epoca {epoch+1}: {time.time() - epoch_start_time:.2f}s\")\n",
    "\n",
    "    print(f\"\\n --- Entrenamiento finalizado ---\")\n",
    "    print(f\"Mejor Val PrAUC: {best_val_auc:.4f}\")\n",
    "    print(f\"Mejor modelo guardado en: {BEST_MODEL_FULL_PATH}\")\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_csv_path = os.path.join(MODEL_SAVE_PATH, \"training_history.csv\")\n",
    "    history_df.to_csv(history_csv_path, index=False)\n",
    "    print(f\"Historial de entrenamiento en: {history_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da24e1-31ea-430c-871f-c2f6c98613c7",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60e8185f-af05-400d-a107-d267bf993c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empezando entrenamiento con NUM_WORKERS = 14\n",
      "--- Creando WeightedRandomSampler ---\n",
      "--- Creando DataLoaders ---\n",
      "\n",
      " --- Comenzado loop entrenamiento por 30 epochs ---\n",
      "Mejor modelo se guardara como: models/edgenext_last_layer/edgenext_best.pth\n",
      "\n",
      "===== Epoch 1/30 =====\n",
      "Learning rate: 1.000000e-04\n",
      "  Epoch [1/30] Batch [50/9467] Train Loss: 0.3627\n",
      "  Epoch [1/30] Batch [100/9467] Train Loss: 0.2684\n",
      "  Epoch [1/30] Batch [150/9467] Train Loss: 0.1854\n",
      "  Epoch [1/30] Batch [200/9467] Train Loss: 0.2518\n",
      "  Epoch [1/30] Batch [250/9467] Train Loss: 0.2246\n",
      "  Epoch [1/30] Batch [300/9467] Train Loss: 0.2097\n",
      "  Epoch [1/30] Batch [350/9467] Train Loss: 0.1995\n",
      "  Epoch [1/30] Batch [400/9467] Train Loss: 0.1429\n",
      "  Epoch [1/30] Batch [450/9467] Train Loss: 0.3667\n",
      "  Epoch [1/30] Batch [500/9467] Train Loss: 0.3118\n",
      "  Epoch [1/30] Batch [550/9467] Train Loss: 0.2731\n",
      "  Epoch [1/30] Batch [600/9467] Train Loss: 0.1279\n",
      "  Epoch [1/30] Batch [650/9467] Train Loss: 0.1465\n",
      "  Epoch [1/30] Batch [700/9467] Train Loss: 0.1335\n",
      "  Epoch [1/30] Batch [750/9467] Train Loss: 0.2072\n",
      "  Epoch [1/30] Batch [800/9467] Train Loss: 0.1101\n",
      "  Epoch [1/30] Batch [850/9467] Train Loss: 0.1554\n",
      "  Epoch [1/30] Batch [900/9467] Train Loss: 0.3360\n",
      "  Epoch [1/30] Batch [950/9467] Train Loss: 0.1600\n",
      "  Epoch [1/30] Batch [1000/9467] Train Loss: 0.2141\n",
      "  Epoch [1/30] Batch [1050/9467] Train Loss: 0.1234\n",
      "  Epoch [1/30] Batch [1100/9467] Train Loss: 0.1991\n",
      "  Epoch [1/30] Batch [1150/9467] Train Loss: 0.1005\n",
      "  Epoch [1/30] Batch [1200/9467] Train Loss: 0.1371\n",
      "  Epoch [1/30] Batch [1250/9467] Train Loss: 0.1756\n",
      "  Epoch [1/30] Batch [1300/9467] Train Loss: 0.1372\n",
      "  Epoch [1/30] Batch [1350/9467] Train Loss: 0.1754\n",
      "  Epoch [1/30] Batch [1400/9467] Train Loss: 0.1419\n",
      "  Epoch [1/30] Batch [1450/9467] Train Loss: 0.0520\n",
      "  Epoch [1/30] Batch [1500/9467] Train Loss: 0.1382\n",
      "  Epoch [1/30] Batch [1550/9467] Train Loss: 0.1063\n",
      "  Epoch [1/30] Batch [1600/9467] Train Loss: 0.0962\n",
      "  Epoch [1/30] Batch [1650/9467] Train Loss: 0.1127\n",
      "  Epoch [1/30] Batch [1700/9467] Train Loss: 0.1072\n",
      "  Epoch [1/30] Batch [1750/9467] Train Loss: 0.1044\n",
      "  Epoch [1/30] Batch [1800/9467] Train Loss: 0.0991\n",
      "  Epoch [1/30] Batch [1850/9467] Train Loss: 0.0890\n",
      "  Epoch [1/30] Batch [1900/9467] Train Loss: 0.0586\n",
      "  Epoch [1/30] Batch [1950/9467] Train Loss: 0.1382\n",
      "  Epoch [1/30] Batch [2000/9467] Train Loss: 0.1908\n",
      "  Epoch [1/30] Batch [2050/9467] Train Loss: 0.1035\n",
      "  Epoch [1/30] Batch [2100/9467] Train Loss: 0.0621\n",
      "  Epoch [1/30] Batch [2150/9467] Train Loss: 0.0996\n",
      "  Epoch [1/30] Batch [2200/9467] Train Loss: 0.0932\n",
      "  Epoch [1/30] Batch [2250/9467] Train Loss: 0.1079\n",
      "  Epoch [1/30] Batch [2300/9467] Train Loss: 0.0607\n",
      "  Epoch [1/30] Batch [2350/9467] Train Loss: 0.1371\n",
      "  Epoch [1/30] Batch [2400/9467] Train Loss: 0.1822\n",
      "  Epoch [1/30] Batch [2450/9467] Train Loss: 0.1141\n",
      "  Epoch [1/30] Batch [2500/9467] Train Loss: 0.1227\n",
      "  Epoch [1/30] Batch [2550/9467] Train Loss: 0.0650\n",
      "  Epoch [1/30] Batch [2600/9467] Train Loss: 0.1345\n",
      "  Epoch [1/30] Batch [2650/9467] Train Loss: 0.0987\n",
      "  Epoch [1/30] Batch [2700/9467] Train Loss: 0.1749\n",
      "  Epoch [1/30] Batch [2750/9467] Train Loss: 0.0587\n",
      "  Epoch [1/30] Batch [2800/9467] Train Loss: 0.0965\n",
      "  Epoch [1/30] Batch [2850/9467] Train Loss: 0.0393\n",
      "  Epoch [1/30] Batch [2900/9467] Train Loss: 0.0798\n",
      "  Epoch [1/30] Batch [2950/9467] Train Loss: 0.0975\n",
      "  Epoch [1/30] Batch [3000/9467] Train Loss: 0.0825\n",
      "  Epoch [1/30] Batch [3050/9467] Train Loss: 0.0770\n",
      "  Epoch [1/30] Batch [3100/9467] Train Loss: 0.1202\n",
      "  Epoch [1/30] Batch [3150/9467] Train Loss: 0.0959\n",
      "  Epoch [1/30] Batch [3200/9467] Train Loss: 0.0963\n",
      "  Epoch [1/30] Batch [3250/9467] Train Loss: 0.0717\n",
      "  Epoch [1/30] Batch [3300/9467] Train Loss: 0.1905\n",
      "  Epoch [1/30] Batch [3350/9467] Train Loss: 0.0720\n",
      "  Epoch [1/30] Batch [3400/9467] Train Loss: 0.1016\n",
      "  Epoch [1/30] Batch [3450/9467] Train Loss: 0.0583\n",
      "  Epoch [1/30] Batch [3500/9467] Train Loss: 0.0641\n",
      "  Epoch [1/30] Batch [3550/9467] Train Loss: 0.0871\n",
      "  Epoch [1/30] Batch [3600/9467] Train Loss: 0.0520\n",
      "  Epoch [1/30] Batch [3650/9467] Train Loss: 0.0486\n",
      "  Epoch [1/30] Batch [3700/9467] Train Loss: 0.0567\n",
      "  Epoch [1/30] Batch [3750/9467] Train Loss: 0.0709\n",
      "  Epoch [1/30] Batch [3800/9467] Train Loss: 0.1798\n",
      "  Epoch [1/30] Batch [3850/9467] Train Loss: 0.0578\n",
      "  Epoch [1/30] Batch [3900/9467] Train Loss: 0.1765\n",
      "  Epoch [1/30] Batch [3950/9467] Train Loss: 0.0515\n",
      "  Epoch [1/30] Batch [4000/9467] Train Loss: 0.0669\n",
      "  Epoch [1/30] Batch [4050/9467] Train Loss: 0.0439\n",
      "  Epoch [1/30] Batch [4100/9467] Train Loss: 0.1092\n",
      "  Epoch [1/30] Batch [4150/9467] Train Loss: 0.0989\n",
      "  Epoch [1/30] Batch [4200/9467] Train Loss: 0.1011\n",
      "  Epoch [1/30] Batch [4250/9467] Train Loss: 0.0390\n",
      "  Epoch [1/30] Batch [4300/9467] Train Loss: 0.0730\n",
      "  Epoch [1/30] Batch [4350/9467] Train Loss: 0.0460\n",
      "  Epoch [1/30] Batch [4400/9467] Train Loss: 0.0193\n",
      "  Epoch [1/30] Batch [4450/9467] Train Loss: 0.0893\n",
      "  Epoch [1/30] Batch [4500/9467] Train Loss: 0.0532\n",
      "  Epoch [1/30] Batch [4550/9467] Train Loss: 0.0737\n",
      "  Epoch [1/30] Batch [4600/9467] Train Loss: 0.0703\n",
      "  Epoch [1/30] Batch [4650/9467] Train Loss: 0.0530\n",
      "  Epoch [1/30] Batch [4700/9467] Train Loss: 0.0487\n",
      "  Epoch [1/30] Batch [4750/9467] Train Loss: 0.1828\n",
      "  Epoch [1/30] Batch [4800/9467] Train Loss: 0.0665\n",
      "  Epoch [1/30] Batch [4850/9467] Train Loss: 0.0428\n",
      "  Epoch [1/30] Batch [4900/9467] Train Loss: 0.0369\n",
      "  Epoch [1/30] Batch [4950/9467] Train Loss: 0.1307\n",
      "  Epoch [1/30] Batch [5000/9467] Train Loss: 0.0618\n",
      "  Epoch [1/30] Batch [5050/9467] Train Loss: 0.0918\n",
      "  Epoch [1/30] Batch [5100/9467] Train Loss: 0.0580\n",
      "  Epoch [1/30] Batch [5150/9467] Train Loss: 0.0370\n",
      "  Epoch [1/30] Batch [5200/9467] Train Loss: 0.0293\n",
      "  Epoch [1/30] Batch [5250/9467] Train Loss: 0.0448\n",
      "  Epoch [1/30] Batch [5300/9467] Train Loss: 0.0332\n",
      "  Epoch [1/30] Batch [5350/9467] Train Loss: 0.0480\n",
      "  Epoch [1/30] Batch [5400/9467] Train Loss: 0.0817\n",
      "  Epoch [1/30] Batch [5450/9467] Train Loss: 0.0256\n",
      "  Epoch [1/30] Batch [5500/9467] Train Loss: 0.0403\n",
      "  Epoch [1/30] Batch [5550/9467] Train Loss: 0.0694\n",
      "  Epoch [1/30] Batch [5600/9467] Train Loss: 0.0555\n",
      "  Epoch [1/30] Batch [5650/9467] Train Loss: 0.0753\n",
      "  Epoch [1/30] Batch [5700/9467] Train Loss: 0.0341\n",
      "  Epoch [1/30] Batch [5750/9467] Train Loss: 0.1276\n",
      "  Epoch [1/30] Batch [5800/9467] Train Loss: 0.1852\n",
      "  Epoch [1/30] Batch [5850/9467] Train Loss: 0.0775\n",
      "  Epoch [1/30] Batch [5900/9467] Train Loss: 0.1203\n",
      "  Epoch [1/30] Batch [5950/9467] Train Loss: 0.1326\n",
      "  Epoch [1/30] Batch [6000/9467] Train Loss: 0.0220\n",
      "  Epoch [1/30] Batch [6050/9467] Train Loss: 0.1151\n",
      "  Epoch [1/30] Batch [6100/9467] Train Loss: 0.0721\n",
      "  Epoch [1/30] Batch [6150/9467] Train Loss: 0.0821\n",
      "  Epoch [1/30] Batch [6200/9467] Train Loss: 0.1314\n",
      "  Epoch [1/30] Batch [6250/9467] Train Loss: 0.0734\n",
      "  Epoch [1/30] Batch [6300/9467] Train Loss: 0.0632\n",
      "  Epoch [1/30] Batch [6350/9467] Train Loss: 0.0157\n",
      "  Epoch [1/30] Batch [6400/9467] Train Loss: 0.0645\n",
      "  Epoch [1/30] Batch [6450/9467] Train Loss: 0.0788\n",
      "  Epoch [1/30] Batch [6500/9467] Train Loss: 0.0606\n",
      "  Epoch [1/30] Batch [6550/9467] Train Loss: 0.0382\n",
      "  Epoch [1/30] Batch [6600/9467] Train Loss: 0.1687\n",
      "  Epoch [1/30] Batch [6650/9467] Train Loss: 0.0484\n",
      "  Epoch [1/30] Batch [6700/9467] Train Loss: 0.0626\n",
      "  Epoch [1/30] Batch [6750/9467] Train Loss: 0.1445\n",
      "  Epoch [1/30] Batch [6800/9467] Train Loss: 0.0719\n",
      "  Epoch [1/30] Batch [6850/9467] Train Loss: 0.0240\n",
      "  Epoch [1/30] Batch [6900/9467] Train Loss: 0.0764\n",
      "  Epoch [1/30] Batch [6950/9467] Train Loss: 0.0454\n",
      "  Epoch [1/30] Batch [7000/9467] Train Loss: 0.0496\n",
      "  Epoch [1/30] Batch [7050/9467] Train Loss: 0.1245\n",
      "  Epoch [1/30] Batch [7100/9467] Train Loss: 0.0406\n",
      "  Epoch [1/30] Batch [7150/9467] Train Loss: 0.0751\n",
      "  Epoch [1/30] Batch [7200/9467] Train Loss: 0.1007\n",
      "  Epoch [1/30] Batch [7250/9467] Train Loss: 0.0939\n",
      "  Epoch [1/30] Batch [7300/9467] Train Loss: 0.1031\n",
      "  Epoch [1/30] Batch [7350/9467] Train Loss: 0.1353\n",
      "  Epoch [1/30] Batch [7400/9467] Train Loss: 0.0724\n",
      "  Epoch [1/30] Batch [7450/9467] Train Loss: 0.0960\n",
      "  Epoch [1/30] Batch [7500/9467] Train Loss: 0.0427\n",
      "  Epoch [1/30] Batch [7550/9467] Train Loss: 0.0606\n",
      "  Epoch [1/30] Batch [7600/9467] Train Loss: 0.1040\n",
      "  Epoch [1/30] Batch [7650/9467] Train Loss: 0.0652\n",
      "  Epoch [1/30] Batch [7700/9467] Train Loss: 0.1529\n",
      "  Epoch [1/30] Batch [7750/9467] Train Loss: 0.0450\n",
      "  Epoch [1/30] Batch [7800/9467] Train Loss: 0.0763\n",
      "  Epoch [1/30] Batch [7850/9467] Train Loss: 0.0980\n",
      "  Epoch [1/30] Batch [7900/9467] Train Loss: 0.0906\n",
      "  Epoch [1/30] Batch [7950/9467] Train Loss: 0.0653\n",
      "  Epoch [1/30] Batch [8000/9467] Train Loss: 0.0573\n",
      "  Epoch [1/30] Batch [8050/9467] Train Loss: 0.0538\n",
      "  Epoch [1/30] Batch [8100/9467] Train Loss: 0.1412\n",
      "  Epoch [1/30] Batch [8150/9467] Train Loss: 0.1124\n",
      "  Epoch [1/30] Batch [8200/9467] Train Loss: 0.0635\n",
      "  Epoch [1/30] Batch [8250/9467] Train Loss: 0.1172\n",
      "  Epoch [1/30] Batch [8300/9467] Train Loss: 0.1249\n",
      "  Epoch [1/30] Batch [8350/9467] Train Loss: 0.0647\n",
      "  Epoch [1/30] Batch [8400/9467] Train Loss: 0.0340\n",
      "  Epoch [1/30] Batch [8450/9467] Train Loss: 0.0264\n",
      "  Epoch [1/30] Batch [8500/9467] Train Loss: 0.0805\n",
      "  Epoch [1/30] Batch [8550/9467] Train Loss: 0.1208\n",
      "  Epoch [1/30] Batch [8600/9467] Train Loss: 0.0470\n",
      "  Epoch [1/30] Batch [8650/9467] Train Loss: 0.0590\n",
      "  Epoch [1/30] Batch [8700/9467] Train Loss: 0.0563\n",
      "  Epoch [1/30] Batch [8750/9467] Train Loss: 0.0792\n",
      "  Epoch [1/30] Batch [8800/9467] Train Loss: 0.0218\n",
      "  Epoch [1/30] Batch [8850/9467] Train Loss: 0.0515\n",
      "  Epoch [1/30] Batch [8900/9467] Train Loss: 0.0472\n",
      "  Epoch [1/30] Batch [8950/9467] Train Loss: 0.0504\n",
      "  Epoch [1/30] Batch [9000/9467] Train Loss: 0.0346\n",
      "  Epoch [1/30] Batch [9050/9467] Train Loss: 0.0248\n",
      "  Epoch [1/30] Batch [9100/9467] Train Loss: 0.0812\n",
      "  Epoch [1/30] Batch [9150/9467] Train Loss: 0.0782\n",
      "  Epoch [1/30] Batch [9200/9467] Train Loss: 0.0846\n",
      "  Epoch [1/30] Batch [9250/9467] Train Loss: 0.0413\n",
      "  Epoch [1/30] Batch [9300/9467] Train Loss: 0.1046\n",
      "  Epoch [1/30] Batch [9350/9467] Train Loss: 0.0649\n",
      "  Epoch [1/30] Batch [9400/9467] Train Loss: 0.0577\n",
      "  Epoch [1/30] Batch [9450/9467] Train Loss: 0.0592\n",
      "Epoch [1/30] Train Loss: 0.1039, AUC: 0.9824, PrAUC : 0.1920, BalAcc: 0.8629, F1: 0.8784, Recall: 0.9878, Precision: 0.7908, Time: 1082.32s\n",
      "Epoch [1/30] Val Loss: 0.3965, AUC: 0.8653, PrAUC: 0.155769, BalAcc: 0.8014, F1(pos): 0.0091, Recall(pos): 0.7660, Precision(pos): 0.0046, Time: 175.24s\n",
      "Matriz de confusion para epoca 1:\n",
      "[[40234  7847]\n",
      " [   11    36]]\n",
      "Val PrAUC mejoro (0.0000 --> 0.1558). Guardando modelo\n",
      "Duracion epoca 1: 1257.86s\n",
      "\n",
      "===== Epoch 2/30 =====\n",
      "Learning rate: 1.000000e-04\n",
      "  Epoch [2/30] Batch [50/9467] Train Loss: 0.0628\n",
      "  Epoch [2/30] Batch [100/9467] Train Loss: 0.0374\n",
      "  Epoch [2/30] Batch [150/9467] Train Loss: 0.0148\n",
      "  Epoch [2/30] Batch [200/9467] Train Loss: 0.1701\n",
      "  Epoch [2/30] Batch [250/9467] Train Loss: 0.0858\n",
      "  Epoch [2/30] Batch [300/9467] Train Loss: 0.1054\n",
      "  Epoch [2/30] Batch [350/9467] Train Loss: 0.0458\n",
      "  Epoch [2/30] Batch [400/9467] Train Loss: 0.1400\n",
      "  Epoch [2/30] Batch [450/9467] Train Loss: 0.0167\n",
      "  Epoch [2/30] Batch [500/9467] Train Loss: 0.0364\n",
      "  Epoch [2/30] Batch [550/9467] Train Loss: 0.0401\n",
      "  Epoch [2/30] Batch [600/9467] Train Loss: 0.0195\n",
      "  Epoch [2/30] Batch [650/9467] Train Loss: 0.0292\n",
      "  Epoch [2/30] Batch [700/9467] Train Loss: 0.0341\n",
      "  Epoch [2/30] Batch [750/9467] Train Loss: 0.0302\n",
      "  Epoch [2/30] Batch [800/9467] Train Loss: 0.0717\n",
      "  Epoch [2/30] Batch [850/9467] Train Loss: 0.0498\n",
      "  Epoch [2/30] Batch [900/9467] Train Loss: 0.0757\n",
      "  Epoch [2/30] Batch [950/9467] Train Loss: 0.0407\n",
      "  Epoch [2/30] Batch [1000/9467] Train Loss: 0.0407\n",
      "  Epoch [2/30] Batch [1050/9467] Train Loss: 0.0327\n",
      "  Epoch [2/30] Batch [1100/9467] Train Loss: 0.1196\n",
      "  Epoch [2/30] Batch [1150/9467] Train Loss: 0.2010\n",
      "  Epoch [2/30] Batch [1200/9467] Train Loss: 0.1458\n",
      "  Epoch [2/30] Batch [1250/9467] Train Loss: 0.0819\n",
      "  Epoch [2/30] Batch [1300/9467] Train Loss: 0.1089\n",
      "  Epoch [2/30] Batch [1350/9467] Train Loss: 0.1451\n",
      "  Epoch [2/30] Batch [1400/9467] Train Loss: 0.0227\n",
      "  Epoch [2/30] Batch [1450/9467] Train Loss: 0.0583\n",
      "  Epoch [2/30] Batch [1500/9467] Train Loss: 0.0396\n",
      "  Epoch [2/30] Batch [1550/9467] Train Loss: 0.0329\n",
      "  Epoch [2/30] Batch [1600/9467] Train Loss: 0.1470\n",
      "  Epoch [2/30] Batch [1650/9467] Train Loss: 0.0499\n",
      "  Epoch [2/30] Batch [1700/9467] Train Loss: 0.0275\n",
      "  Epoch [2/30] Batch [1750/9467] Train Loss: 0.0515\n",
      "  Epoch [2/30] Batch [1800/9467] Train Loss: 0.0882\n",
      "  Epoch [2/30] Batch [1850/9467] Train Loss: 0.0326\n",
      "  Epoch [2/30] Batch [1900/9467] Train Loss: 0.0351\n",
      "  Epoch [2/30] Batch [1950/9467] Train Loss: 0.0462\n",
      "  Epoch [2/30] Batch [2000/9467] Train Loss: 0.0370\n",
      "  Epoch [2/30] Batch [2050/9467] Train Loss: 0.0829\n",
      "  Epoch [2/30] Batch [2100/9467] Train Loss: 0.0723\n",
      "  Epoch [2/30] Batch [2150/9467] Train Loss: 0.0392\n",
      "  Epoch [2/30] Batch [2200/9467] Train Loss: 0.0330\n",
      "  Epoch [2/30] Batch [2250/9467] Train Loss: 0.0808\n",
      "  Epoch [2/30] Batch [2300/9467] Train Loss: 0.0230\n",
      "  Epoch [2/30] Batch [2350/9467] Train Loss: 0.0668\n",
      "  Epoch [2/30] Batch [2400/9467] Train Loss: 0.0848\n",
      "  Epoch [2/30] Batch [2450/9467] Train Loss: 0.0080\n",
      "  Epoch [2/30] Batch [2500/9467] Train Loss: 0.0953\n",
      "  Epoch [2/30] Batch [2550/9467] Train Loss: 0.0304\n",
      "  Epoch [2/30] Batch [2600/9467] Train Loss: 0.0978\n",
      "  Epoch [2/30] Batch [2650/9467] Train Loss: 0.0872\n",
      "  Epoch [2/30] Batch [2700/9467] Train Loss: 0.0681\n",
      "  Epoch [2/30] Batch [2750/9467] Train Loss: 0.0414\n",
      "  Epoch [2/30] Batch [2800/9467] Train Loss: 0.0690\n",
      "  Epoch [2/30] Batch [2850/9467] Train Loss: 0.0610\n",
      "  Epoch [2/30] Batch [2900/9467] Train Loss: 0.0341\n",
      "  Epoch [2/30] Batch [2950/9467] Train Loss: 0.1111\n",
      "  Epoch [2/30] Batch [3000/9467] Train Loss: 0.0394\n",
      "  Epoch [2/30] Batch [3050/9467] Train Loss: 0.0409\n",
      "  Epoch [2/30] Batch [3100/9467] Train Loss: 0.0819\n",
      "  Epoch [2/30] Batch [3150/9467] Train Loss: 0.1468\n",
      "  Epoch [2/30] Batch [3200/9467] Train Loss: 0.0714\n",
      "  Epoch [2/30] Batch [3250/9467] Train Loss: 0.0228\n",
      "  Epoch [2/30] Batch [3300/9467] Train Loss: 0.0596\n",
      "  Epoch [2/30] Batch [3350/9467] Train Loss: 0.0416\n",
      "  Epoch [2/30] Batch [3400/9467] Train Loss: 0.0278\n",
      "  Epoch [2/30] Batch [3450/9467] Train Loss: 0.0748\n",
      "  Epoch [2/30] Batch [3500/9467] Train Loss: 0.0302\n",
      "  Epoch [2/30] Batch [3550/9467] Train Loss: 0.0278\n",
      "  Epoch [2/30] Batch [3600/9467] Train Loss: 0.1559\n",
      "  Epoch [2/30] Batch [3650/9467] Train Loss: 0.0443\n",
      "  Epoch [2/30] Batch [3700/9467] Train Loss: 0.0301\n",
      "  Epoch [2/30] Batch [3750/9467] Train Loss: 0.0499\n",
      "  Epoch [2/30] Batch [3800/9467] Train Loss: 0.0329\n",
      "  Epoch [2/30] Batch [3850/9467] Train Loss: 0.0355\n",
      "  Epoch [2/30] Batch [3900/9467] Train Loss: 0.0568\n",
      "  Epoch [2/30] Batch [3950/9467] Train Loss: 0.0519\n",
      "  Epoch [2/30] Batch [4000/9467] Train Loss: 0.0488\n",
      "  Epoch [2/30] Batch [4050/9467] Train Loss: 0.0488\n",
      "  Epoch [2/30] Batch [4100/9467] Train Loss: 0.0195\n",
      "  Epoch [2/30] Batch [4150/9467] Train Loss: 0.0566\n",
      "  Epoch [2/30] Batch [4200/9467] Train Loss: 0.0594\n",
      "  Epoch [2/30] Batch [4250/9467] Train Loss: 0.1199\n",
      "  Epoch [2/30] Batch [4300/9467] Train Loss: 0.0594\n",
      "  Epoch [2/30] Batch [4350/9467] Train Loss: 0.0340\n",
      "  Epoch [2/30] Batch [4400/9467] Train Loss: 0.0362\n",
      "  Epoch [2/30] Batch [4450/9467] Train Loss: 0.0463\n",
      "  Epoch [2/30] Batch [4500/9467] Train Loss: 0.0946\n",
      "  Epoch [2/30] Batch [4550/9467] Train Loss: 0.2287\n",
      "  Epoch [2/30] Batch [4600/9467] Train Loss: 0.1285\n",
      "  Epoch [2/30] Batch [4650/9467] Train Loss: 0.0827\n",
      "  Epoch [2/30] Batch [4700/9467] Train Loss: 0.1625\n",
      "  Epoch [2/30] Batch [4750/9467] Train Loss: 0.0958\n",
      "  Epoch [2/30] Batch [4800/9467] Train Loss: 0.0547\n",
      "  Epoch [2/30] Batch [4850/9467] Train Loss: 0.0235\n",
      "  Epoch [2/30] Batch [4900/9467] Train Loss: 0.0820\n",
      "  Epoch [2/30] Batch [4950/9467] Train Loss: 0.0377\n",
      "  Epoch [2/30] Batch [5000/9467] Train Loss: 0.0547\n",
      "  Epoch [2/30] Batch [5050/9467] Train Loss: 0.0432\n",
      "  Epoch [2/30] Batch [5100/9467] Train Loss: 0.0601\n",
      "  Epoch [2/30] Batch [5150/9467] Train Loss: 0.0680\n",
      "  Epoch [2/30] Batch [5200/9467] Train Loss: 0.0259\n",
      "  Epoch [2/30] Batch [5250/9467] Train Loss: 0.0374\n",
      "  Epoch [2/30] Batch [5300/9467] Train Loss: 0.0509\n",
      "  Epoch [2/30] Batch [5350/9467] Train Loss: 0.0478\n",
      "  Epoch [2/30] Batch [5400/9467] Train Loss: 0.0581\n",
      "  Epoch [2/30] Batch [5450/9467] Train Loss: 0.0604\n",
      "  Epoch [2/30] Batch [5500/9467] Train Loss: 0.0285\n",
      "  Epoch [2/30] Batch [5550/9467] Train Loss: 0.0272\n",
      "  Epoch [2/30] Batch [5600/9467] Train Loss: 0.0453\n",
      "  Epoch [2/30] Batch [5650/9467] Train Loss: 0.1054\n",
      "  Epoch [2/30] Batch [5700/9467] Train Loss: 0.1273\n",
      "  Epoch [2/30] Batch [5750/9467] Train Loss: 0.0199\n",
      "  Epoch [2/30] Batch [5800/9467] Train Loss: 0.0390\n",
      "  Epoch [2/30] Batch [5850/9467] Train Loss: 0.0557\n",
      "  Epoch [2/30] Batch [5900/9467] Train Loss: 0.0594\n",
      "  Epoch [2/30] Batch [5950/9467] Train Loss: 0.0556\n",
      "  Epoch [2/30] Batch [6000/9467] Train Loss: 0.1108\n",
      "  Epoch [2/30] Batch [6050/9467] Train Loss: 0.1483\n",
      "  Epoch [2/30] Batch [6100/9467] Train Loss: 0.0238\n",
      "  Epoch [2/30] Batch [6150/9467] Train Loss: 0.0445\n",
      "  Epoch [2/30] Batch [6200/9467] Train Loss: 0.0401\n",
      "  Epoch [2/30] Batch [6250/9467] Train Loss: 0.0142\n",
      "  Epoch [2/30] Batch [6300/9467] Train Loss: 0.0439\n",
      "  Epoch [2/30] Batch [6350/9467] Train Loss: 0.0236\n",
      "  Epoch [2/30] Batch [6400/9467] Train Loss: 0.2064\n",
      "  Epoch [2/30] Batch [6450/9467] Train Loss: 0.0216\n",
      "  Epoch [2/30] Batch [6500/9467] Train Loss: 0.0501\n",
      "  Epoch [2/30] Batch [6550/9467] Train Loss: 0.0539\n",
      "  Epoch [2/30] Batch [6600/9467] Train Loss: 0.1170\n",
      "  Epoch [2/30] Batch [6650/9467] Train Loss: 0.0904\n",
      "  Epoch [2/30] Batch [6700/9467] Train Loss: 0.0192\n",
      "  Epoch [2/30] Batch [6750/9467] Train Loss: 0.0550\n",
      "  Epoch [2/30] Batch [6800/9467] Train Loss: 0.0267\n",
      "  Epoch [2/30] Batch [6850/9467] Train Loss: 0.0519\n",
      "  Epoch [2/30] Batch [6900/9467] Train Loss: 0.1005\n",
      "  Epoch [2/30] Batch [6950/9467] Train Loss: 0.0473\n",
      "  Epoch [2/30] Batch [7000/9467] Train Loss: 0.0470\n",
      "  Epoch [2/30] Batch [7050/9467] Train Loss: 0.0556\n",
      "  Epoch [2/30] Batch [7100/9467] Train Loss: 0.1154\n",
      "  Epoch [2/30] Batch [7150/9467] Train Loss: 0.0896\n",
      "  Epoch [2/30] Batch [7200/9467] Train Loss: 0.0583\n",
      "  Epoch [2/30] Batch [7250/9467] Train Loss: 0.0911\n",
      "  Epoch [2/30] Batch [7300/9467] Train Loss: 0.0768\n",
      "  Epoch [2/30] Batch [7350/9467] Train Loss: 0.0436\n",
      "  Epoch [2/30] Batch [7400/9467] Train Loss: 0.0360\n",
      "  Epoch [2/30] Batch [7450/9467] Train Loss: 0.0576\n",
      "  Epoch [2/30] Batch [7500/9467] Train Loss: 0.0277\n",
      "  Epoch [2/30] Batch [7550/9467] Train Loss: 0.0410\n",
      "  Epoch [2/30] Batch [7600/9467] Train Loss: 0.0395\n",
      "  Epoch [2/30] Batch [7650/9467] Train Loss: 0.0949\n",
      "  Epoch [2/30] Batch [7700/9467] Train Loss: 0.0366\n",
      "  Epoch [2/30] Batch [7750/9467] Train Loss: 0.0864\n",
      "  Epoch [2/30] Batch [7800/9467] Train Loss: 0.0174\n",
      "  Epoch [2/30] Batch [7850/9467] Train Loss: 0.0358\n",
      "  Epoch [2/30] Batch [7900/9467] Train Loss: 0.0694\n",
      "  Epoch [2/30] Batch [7950/9467] Train Loss: 0.0223\n",
      "  Epoch [2/30] Batch [8000/9467] Train Loss: 0.0171\n",
      "  Epoch [2/30] Batch [8050/9467] Train Loss: 0.1702\n",
      "  Epoch [2/30] Batch [8100/9467] Train Loss: 0.0499\n",
      "  Epoch [2/30] Batch [8150/9467] Train Loss: 0.0223\n",
      "  Epoch [2/30] Batch [8200/9467] Train Loss: 0.0275\n",
      "  Epoch [2/30] Batch [8250/9467] Train Loss: 0.1168\n",
      "  Epoch [2/30] Batch [8300/9467] Train Loss: 0.1924\n",
      "  Epoch [2/30] Batch [8350/9467] Train Loss: 0.0665\n",
      "  Epoch [2/30] Batch [8400/9467] Train Loss: 0.0274\n",
      "  Epoch [2/30] Batch [8450/9467] Train Loss: 0.0575\n",
      "  Epoch [2/30] Batch [8500/9467] Train Loss: 0.0388\n",
      "  Epoch [2/30] Batch [8550/9467] Train Loss: 0.0497\n",
      "  Epoch [2/30] Batch [8600/9467] Train Loss: 0.2217\n",
      "  Epoch [2/30] Batch [8650/9467] Train Loss: 0.0583\n",
      "  Epoch [2/30] Batch [8700/9467] Train Loss: 0.1033\n",
      "  Epoch [2/30] Batch [8750/9467] Train Loss: 0.0589\n",
      "  Epoch [2/30] Batch [8800/9467] Train Loss: 0.0517\n",
      "  Epoch [2/30] Batch [8850/9467] Train Loss: 0.0164\n",
      "  Epoch [2/30] Batch [8900/9467] Train Loss: 0.0427\n",
      "  Epoch [2/30] Batch [8950/9467] Train Loss: 0.0291\n",
      "  Epoch [2/30] Batch [9000/9467] Train Loss: 0.0238\n",
      "  Epoch [2/30] Batch [9050/9467] Train Loss: 0.0356\n",
      "  Epoch [2/30] Batch [9100/9467] Train Loss: 0.0158\n",
      "  Epoch [2/30] Batch [9150/9467] Train Loss: 0.0414\n",
      "  Epoch [2/30] Batch [9200/9467] Train Loss: 0.0275\n",
      "  Epoch [2/30] Batch [9250/9467] Train Loss: 0.0546\n",
      "  Epoch [2/30] Batch [9300/9467] Train Loss: 0.0446\n",
      "  Epoch [2/30] Batch [9350/9467] Train Loss: 0.0323\n",
      "  Epoch [2/30] Batch [9400/9467] Train Loss: 0.1024\n",
      "  Epoch [2/30] Batch [9450/9467] Train Loss: 0.0192\n",
      "Epoch [2/30] Train Loss: 0.0633, AUC: 0.9931, PrAUC : 0.1968, BalAcc: 0.9311, F1: 0.9351, Recall: 0.9907, Precision: 0.8854, Time: 1110.96s\n",
      "Epoch [2/30] Val Loss: 0.3306, AUC: 0.8594, PrAUC: 0.153417, BalAcc: 0.7642, F1(pos): 0.0097, Recall(pos): 0.6596, Precision(pos): 0.0049, Time: 185.90s\n",
      "Matriz de confusion para epoca 2:\n",
      "[[41770  6311]\n",
      " [   16    31]]\n",
      "Val AUC (0.1534) no mejoro desde la mejor (0.1558). Ninguna mejora durante 1 epochs.\n",
      "Duracion epoca 2: 1296.89s\n",
      "\n",
      "===== Epoch 3/30 =====\n",
      "Learning rate: 1.000000e-04\n",
      "  Epoch [3/30] Batch [50/9467] Train Loss: 0.0743\n",
      "  Epoch [3/30] Batch [100/9467] Train Loss: 0.0360\n",
      "  Epoch [3/30] Batch [150/9467] Train Loss: 0.0210\n",
      "  Epoch [3/30] Batch [200/9467] Train Loss: 0.0606\n",
      "  Epoch [3/30] Batch [250/9467] Train Loss: 0.0274\n",
      "  Epoch [3/30] Batch [300/9467] Train Loss: 0.0643\n",
      "  Epoch [3/30] Batch [350/9467] Train Loss: 0.0211\n",
      "  Epoch [3/30] Batch [400/9467] Train Loss: 0.0531\n",
      "  Epoch [3/30] Batch [450/9467] Train Loss: 0.0204\n",
      "  Epoch [3/30] Batch [500/9467] Train Loss: 0.0209\n",
      "  Epoch [3/30] Batch [550/9467] Train Loss: 0.0323\n",
      "  Epoch [3/30] Batch [600/9467] Train Loss: 0.1065\n",
      "  Epoch [3/30] Batch [650/9467] Train Loss: 0.0724\n",
      "  Epoch [3/30] Batch [700/9467] Train Loss: 0.0441\n",
      "  Epoch [3/30] Batch [750/9467] Train Loss: 0.0397\n",
      "  Epoch [3/30] Batch [800/9467] Train Loss: 0.0348\n",
      "  Epoch [3/30] Batch [850/9467] Train Loss: 0.0399\n",
      "  Epoch [3/30] Batch [900/9467] Train Loss: 0.0617\n",
      "  Epoch [3/30] Batch [950/9467] Train Loss: 0.0244\n",
      "  Epoch [3/30] Batch [1000/9467] Train Loss: 0.0281\n",
      "  Epoch [3/30] Batch [1050/9467] Train Loss: 0.0656\n",
      "  Epoch [3/30] Batch [1100/9467] Train Loss: 0.0353\n",
      "  Epoch [3/30] Batch [1150/9467] Train Loss: 0.0982\n",
      "  Epoch [3/30] Batch [1200/9467] Train Loss: 0.0664\n",
      "  Epoch [3/30] Batch [1250/9467] Train Loss: 0.1080\n",
      "  Epoch [3/30] Batch [1300/9467] Train Loss: 0.0476\n",
      "  Epoch [3/30] Batch [1350/9467] Train Loss: 0.0251\n",
      "  Epoch [3/30] Batch [1400/9467] Train Loss: 0.0178\n",
      "  Epoch [3/30] Batch [1450/9467] Train Loss: 0.0171\n",
      "  Epoch [3/30] Batch [1500/9467] Train Loss: 0.0379\n",
      "  Epoch [3/30] Batch [1550/9467] Train Loss: 0.0818\n",
      "  Epoch [3/30] Batch [1600/9467] Train Loss: 0.0486\n",
      "  Epoch [3/30] Batch [1650/9467] Train Loss: 0.0257\n",
      "  Epoch [3/30] Batch [1700/9467] Train Loss: 0.0242\n",
      "  Epoch [3/30] Batch [1750/9467] Train Loss: 0.0501\n",
      "  Epoch [3/30] Batch [1800/9467] Train Loss: 0.1030\n",
      "  Epoch [3/30] Batch [1850/9467] Train Loss: 0.1664\n",
      "  Epoch [3/30] Batch [1900/9467] Train Loss: 0.0480\n",
      "  Epoch [3/30] Batch [1950/9467] Train Loss: 0.0360\n",
      "  Epoch [3/30] Batch [2000/9467] Train Loss: 0.0428\n",
      "  Epoch [3/30] Batch [2050/9467] Train Loss: 0.0512\n",
      "  Epoch [3/30] Batch [2100/9467] Train Loss: 0.0432\n",
      "  Epoch [3/30] Batch [2150/9467] Train Loss: 0.0482\n",
      "  Epoch [3/30] Batch [2200/9467] Train Loss: 0.0542\n",
      "  Epoch [3/30] Batch [2250/9467] Train Loss: 0.0200\n",
      "  Epoch [3/30] Batch [2300/9467] Train Loss: 0.0621\n",
      "  Epoch [3/30] Batch [2350/9467] Train Loss: 0.0164\n",
      "  Epoch [3/30] Batch [2400/9467] Train Loss: 0.0313\n",
      "  Epoch [3/30] Batch [2450/9467] Train Loss: 0.1053\n",
      "  Epoch [3/30] Batch [2500/9467] Train Loss: 0.0993\n",
      "  Epoch [3/30] Batch [2550/9467] Train Loss: 0.0384\n",
      "  Epoch [3/30] Batch [2600/9467] Train Loss: 0.1306\n",
      "  Epoch [3/30] Batch [2650/9467] Train Loss: 0.0607\n",
      "  Epoch [3/30] Batch [2700/9467] Train Loss: 0.0136\n",
      "  Epoch [3/30] Batch [2750/9467] Train Loss: 0.0285\n",
      "  Epoch [3/30] Batch [2800/9467] Train Loss: 0.0449\n",
      "  Epoch [3/30] Batch [2850/9467] Train Loss: 0.2045\n",
      "  Epoch [3/30] Batch [2900/9467] Train Loss: 0.1998\n",
      "  Epoch [3/30] Batch [2950/9467] Train Loss: 0.0447\n",
      "  Epoch [3/30] Batch [3000/9467] Train Loss: 0.0807\n",
      "  Epoch [3/30] Batch [3050/9467] Train Loss: 0.0599\n",
      "  Epoch [3/30] Batch [3100/9467] Train Loss: 0.0336\n",
      "  Epoch [3/30] Batch [3150/9467] Train Loss: 0.0944\n",
      "  Epoch [3/30] Batch [3200/9467] Train Loss: 0.0230\n",
      "  Epoch [3/30] Batch [3250/9467] Train Loss: 0.0694\n",
      "  Epoch [3/30] Batch [3300/9467] Train Loss: 0.0422\n",
      "  Epoch [3/30] Batch [3350/9467] Train Loss: 0.0854\n",
      "  Epoch [3/30] Batch [3400/9467] Train Loss: 0.0423\n",
      "  Epoch [3/30] Batch [3450/9467] Train Loss: 0.0995\n",
      "  Epoch [3/30] Batch [3500/9467] Train Loss: 0.0277\n",
      "  Epoch [3/30] Batch [3550/9467] Train Loss: 0.0583\n",
      "  Epoch [3/30] Batch [3600/9467] Train Loss: 0.0281\n",
      "  Epoch [3/30] Batch [3650/9467] Train Loss: 0.0422\n",
      "  Epoch [3/30] Batch [3700/9467] Train Loss: 0.0647\n",
      "  Epoch [3/30] Batch [3750/9467] Train Loss: 0.0360\n",
      "  Epoch [3/30] Batch [3800/9467] Train Loss: 0.1010\n",
      "  Epoch [3/30] Batch [3850/9467] Train Loss: 0.0285\n",
      "  Epoch [3/30] Batch [3900/9467] Train Loss: 0.0255\n",
      "  Epoch [3/30] Batch [3950/9467] Train Loss: 0.0198\n",
      "  Epoch [3/30] Batch [4000/9467] Train Loss: 0.0632\n",
      "  Epoch [3/30] Batch [4050/9467] Train Loss: 0.0328\n",
      "  Epoch [3/30] Batch [4100/9467] Train Loss: 0.0495\n",
      "  Epoch [3/30] Batch [4150/9467] Train Loss: 0.0417\n",
      "  Epoch [3/30] Batch [4200/9467] Train Loss: 0.0358\n",
      "  Epoch [3/30] Batch [4250/9467] Train Loss: 0.0369\n",
      "  Epoch [3/30] Batch [4300/9467] Train Loss: 0.0657\n",
      "  Epoch [3/30] Batch [4350/9467] Train Loss: 0.0290\n",
      "  Epoch [3/30] Batch [4400/9467] Train Loss: 0.0566\n",
      "  Epoch [3/30] Batch [4450/9467] Train Loss: 0.1051\n",
      "  Epoch [3/30] Batch [4500/9467] Train Loss: 0.0443\n",
      "  Epoch [3/30] Batch [4550/9467] Train Loss: 0.1121\n",
      "  Epoch [3/30] Batch [4600/9467] Train Loss: 0.0162\n",
      "  Epoch [3/30] Batch [4650/9467] Train Loss: 0.0740\n",
      "  Epoch [3/30] Batch [4700/9467] Train Loss: 0.0234\n",
      "  Epoch [3/30] Batch [4750/9467] Train Loss: 0.0390\n",
      "  Epoch [3/30] Batch [4800/9467] Train Loss: 0.0614\n",
      "  Epoch [3/30] Batch [4850/9467] Train Loss: 0.0364\n",
      "  Epoch [3/30] Batch [4900/9467] Train Loss: 0.0418\n",
      "  Epoch [3/30] Batch [4950/9467] Train Loss: 0.0608\n",
      "  Epoch [3/30] Batch [5000/9467] Train Loss: 0.0453\n",
      "  Epoch [3/30] Batch [5050/9467] Train Loss: 0.0227\n",
      "  Epoch [3/30] Batch [5100/9467] Train Loss: 0.0184\n",
      "  Epoch [3/30] Batch [5150/9467] Train Loss: 0.1227\n",
      "  Epoch [3/30] Batch [5200/9467] Train Loss: 0.0427\n",
      "  Epoch [3/30] Batch [5250/9467] Train Loss: 0.0406\n",
      "  Epoch [3/30] Batch [5300/9467] Train Loss: 0.0476\n",
      "  Epoch [3/30] Batch [5350/9467] Train Loss: 0.1240\n",
      "  Epoch [3/30] Batch [5400/9467] Train Loss: 0.0175\n",
      "  Epoch [3/30] Batch [5450/9467] Train Loss: 0.0630\n",
      "  Epoch [3/30] Batch [5500/9467] Train Loss: 0.0525\n",
      "  Epoch [3/30] Batch [5550/9467] Train Loss: 0.0311\n",
      "  Epoch [3/30] Batch [5600/9467] Train Loss: 0.0231\n",
      "  Epoch [3/30] Batch [5650/9467] Train Loss: 0.0239\n",
      "  Epoch [3/30] Batch [5700/9467] Train Loss: 0.0518\n",
      "  Epoch [3/30] Batch [5750/9467] Train Loss: 0.0975\n",
      "  Epoch [3/30] Batch [5800/9467] Train Loss: 0.0260\n",
      "  Epoch [3/30] Batch [5850/9467] Train Loss: 0.0533\n",
      "  Epoch [3/30] Batch [5900/9467] Train Loss: 0.0799\n",
      "  Epoch [3/30] Batch [5950/9467] Train Loss: 0.1106\n",
      "  Epoch [3/30] Batch [6000/9467] Train Loss: 0.0236\n",
      "  Epoch [3/30] Batch [6050/9467] Train Loss: 0.0993\n",
      "  Epoch [3/30] Batch [6100/9467] Train Loss: 0.0718\n",
      "  Epoch [3/30] Batch [6150/9467] Train Loss: 0.0180\n",
      "  Epoch [3/30] Batch [6200/9467] Train Loss: 0.0185\n",
      "  Epoch [3/30] Batch [6250/9467] Train Loss: 0.0440\n",
      "  Epoch [3/30] Batch [6300/9467] Train Loss: 0.0180\n",
      "  Epoch [3/30] Batch [6350/9467] Train Loss: 0.0091\n",
      "  Epoch [3/30] Batch [6400/9467] Train Loss: 0.0427\n",
      "  Epoch [3/30] Batch [6450/9467] Train Loss: 0.0165\n",
      "  Epoch [3/30] Batch [6500/9467] Train Loss: 0.0203\n",
      "  Epoch [3/30] Batch [6550/9467] Train Loss: 0.0188\n",
      "  Epoch [3/30] Batch [6600/9467] Train Loss: 0.0719\n",
      "  Epoch [3/30] Batch [6650/9467] Train Loss: 0.0855\n",
      "  Epoch [3/30] Batch [6700/9467] Train Loss: 0.0552\n",
      "  Epoch [3/30] Batch [6750/9467] Train Loss: 0.0491\n",
      "  Epoch [3/30] Batch [6800/9467] Train Loss: 0.0541\n",
      "  Epoch [3/30] Batch [6850/9467] Train Loss: 0.0304\n",
      "  Epoch [3/30] Batch [6900/9467] Train Loss: 0.0288\n",
      "  Epoch [3/30] Batch [6950/9467] Train Loss: 0.0271\n",
      "  Epoch [3/30] Batch [7000/9467] Train Loss: 0.0413\n",
      "  Epoch [3/30] Batch [7050/9467] Train Loss: 0.1160\n",
      "  Epoch [3/30] Batch [7100/9467] Train Loss: 0.0517\n",
      "  Epoch [3/30] Batch [7150/9467] Train Loss: 0.0122\n",
      "  Epoch [3/30] Batch [7200/9467] Train Loss: 0.0344\n",
      "  Epoch [3/30] Batch [7250/9467] Train Loss: 0.0689\n",
      "  Epoch [3/30] Batch [7300/9467] Train Loss: 0.0409\n",
      "  Epoch [3/30] Batch [7350/9467] Train Loss: 0.0283\n",
      "  Epoch [3/30] Batch [7400/9467] Train Loss: 0.2585\n",
      "  Epoch [3/30] Batch [7450/9467] Train Loss: 0.0732\n",
      "  Epoch [3/30] Batch [7500/9467] Train Loss: 0.2066\n",
      "  Epoch [3/30] Batch [7550/9467] Train Loss: 0.1199\n",
      "  Epoch [3/30] Batch [7600/9467] Train Loss: 0.0627\n",
      "  Epoch [3/30] Batch [7650/9467] Train Loss: 0.0570\n",
      "  Epoch [3/30] Batch [7700/9467] Train Loss: 0.0983\n",
      "  Epoch [3/30] Batch [7750/9467] Train Loss: 0.0269\n",
      "  Epoch [3/30] Batch [7800/9467] Train Loss: 0.0237\n",
      "  Epoch [3/30] Batch [7850/9467] Train Loss: 0.0315\n",
      "  Epoch [3/30] Batch [7900/9467] Train Loss: 0.0287\n",
      "  Epoch [3/30] Batch [7950/9467] Train Loss: 0.0267\n",
      "  Epoch [3/30] Batch [8000/9467] Train Loss: 0.0386\n",
      "  Epoch [3/30] Batch [8050/9467] Train Loss: 0.0782\n",
      "  Epoch [3/30] Batch [8100/9467] Train Loss: 0.0610\n",
      "  Epoch [3/30] Batch [8150/9467] Train Loss: 0.0640\n",
      "  Epoch [3/30] Batch [8200/9467] Train Loss: 0.0485\n",
      "  Epoch [3/30] Batch [8250/9467] Train Loss: 0.1141\n",
      "  Epoch [3/30] Batch [8300/9467] Train Loss: 0.0316\n",
      "  Epoch [3/30] Batch [8350/9467] Train Loss: 0.0156\n",
      "  Epoch [3/30] Batch [8400/9467] Train Loss: 0.0235\n",
      "  Epoch [3/30] Batch [8450/9467] Train Loss: 0.0743\n",
      "  Epoch [3/30] Batch [8500/9467] Train Loss: 0.0275\n",
      "  Epoch [3/30] Batch [8550/9467] Train Loss: 0.0468\n",
      "  Epoch [3/30] Batch [8600/9467] Train Loss: 0.0187\n",
      "  Epoch [3/30] Batch [8650/9467] Train Loss: 0.0476\n",
      "  Epoch [3/30] Batch [8700/9467] Train Loss: 0.0879\n",
      "  Epoch [3/30] Batch [8750/9467] Train Loss: 0.0750\n",
      "  Epoch [3/30] Batch [8800/9467] Train Loss: 0.0426\n",
      "  Epoch [3/30] Batch [8850/9467] Train Loss: 0.0450\n",
      "  Epoch [3/30] Batch [8900/9467] Train Loss: 0.0197\n",
      "  Epoch [3/30] Batch [8950/9467] Train Loss: 0.2068\n",
      "  Epoch [3/30] Batch [9000/9467] Train Loss: 0.0781\n",
      "  Epoch [3/30] Batch [9050/9467] Train Loss: 0.0524\n",
      "  Epoch [3/30] Batch [9100/9467] Train Loss: 0.0393\n",
      "  Epoch [3/30] Batch [9150/9467] Train Loss: 0.0355\n",
      "  Epoch [3/30] Batch [9200/9467] Train Loss: 0.0401\n",
      "  Epoch [3/30] Batch [9250/9467] Train Loss: 0.0441\n",
      "  Epoch [3/30] Batch [9300/9467] Train Loss: 0.0300\n",
      "  Epoch [3/30] Batch [9350/9467] Train Loss: 0.0331\n",
      "  Epoch [3/30] Batch [9400/9467] Train Loss: 0.0372\n",
      "  Epoch [3/30] Batch [9450/9467] Train Loss: 0.0237\n",
      "Epoch [3/30] Train Loss: 0.0571, AUC: 0.9943, PrAUC : 0.1973, BalAcc: 0.9409, F1: 0.9435, Recall: 0.9913, Precision: 0.9001, Time: 1160.60s\n",
      "Epoch [3/30] Val Loss: 0.3463, AUC: 0.8545, PrAUC: 0.152456, BalAcc: 0.7505, F1(pos): 0.0090, Recall(pos): 0.6383, Precision(pos): 0.0045, Time: 165.16s\n",
      "Matriz de confusion para epoca 3:\n",
      "[[41477  6604]\n",
      " [   17    30]]\n",
      "Val AUC (0.1525) no mejoro desde la mejor (0.1558). Ninguna mejora durante 2 epochs.\n",
      "Duracion epoca 3: 1325.80s\n",
      "\n",
      "===== Epoch 4/30 =====\n",
      "Learning rate: 1.000000e-04\n",
      "  Epoch [4/30] Batch [50/9467] Train Loss: 0.0211\n",
      "  Epoch [4/30] Batch [100/9467] Train Loss: 0.0154\n",
      "  Epoch [4/30] Batch [150/9467] Train Loss: 0.0361\n",
      "  Epoch [4/30] Batch [200/9467] Train Loss: 0.0572\n",
      "  Epoch [4/30] Batch [250/9467] Train Loss: 0.0401\n",
      "  Epoch [4/30] Batch [300/9467] Train Loss: 0.0393\n",
      "  Epoch [4/30] Batch [350/9467] Train Loss: 0.0276\n",
      "  Epoch [4/30] Batch [400/9467] Train Loss: 0.1022\n",
      "  Epoch [4/30] Batch [450/9467] Train Loss: 0.1421\n",
      "  Epoch [4/30] Batch [500/9467] Train Loss: 0.0164\n",
      "  Epoch [4/30] Batch [550/9467] Train Loss: 0.0651\n",
      "  Epoch [4/30] Batch [600/9467] Train Loss: 0.0323\n",
      "  Epoch [4/30] Batch [650/9467] Train Loss: 0.0724\n",
      "  Epoch [4/30] Batch [700/9467] Train Loss: 0.0506\n",
      "  Epoch [4/30] Batch [750/9467] Train Loss: 0.0507\n",
      "  Epoch [4/30] Batch [800/9467] Train Loss: 0.0275\n",
      "  Epoch [4/30] Batch [850/9467] Train Loss: 0.0610\n",
      "  Epoch [4/30] Batch [900/9467] Train Loss: 0.0801\n",
      "  Epoch [4/30] Batch [950/9467] Train Loss: 0.0680\n",
      "  Epoch [4/30] Batch [1000/9467] Train Loss: 0.0325\n",
      "  Epoch [4/30] Batch [1050/9467] Train Loss: 0.0419\n",
      "  Epoch [4/30] Batch [1100/9467] Train Loss: 0.0404\n",
      "  Epoch [4/30] Batch [1150/9467] Train Loss: 0.1445\n",
      "  Epoch [4/30] Batch [1200/9467] Train Loss: 0.1057\n",
      "  Epoch [4/30] Batch [1250/9467] Train Loss: 0.0606\n",
      "  Epoch [4/30] Batch [1300/9467] Train Loss: 0.0333\n",
      "  Epoch [4/30] Batch [1350/9467] Train Loss: 0.0196\n",
      "  Epoch [4/30] Batch [1400/9467] Train Loss: 0.0264\n",
      "  Epoch [4/30] Batch [1450/9467] Train Loss: 0.0238\n",
      "  Epoch [4/30] Batch [1500/9467] Train Loss: 0.0497\n",
      "  Epoch [4/30] Batch [1550/9467] Train Loss: 0.1018\n",
      "  Epoch [4/30] Batch [1600/9467] Train Loss: 0.0377\n",
      "  Epoch [4/30] Batch [1650/9467] Train Loss: 0.0280\n",
      "  Epoch [4/30] Batch [1700/9467] Train Loss: 0.0487\n",
      "  Epoch [4/30] Batch [1750/9467] Train Loss: 0.0391\n",
      "  Epoch [4/30] Batch [1800/9467] Train Loss: 0.0195\n",
      "  Epoch [4/30] Batch [1850/9467] Train Loss: 0.0207\n",
      "  Epoch [4/30] Batch [1900/9467] Train Loss: 0.0780\n",
      "  Epoch [4/30] Batch [1950/9467] Train Loss: 0.0410\n",
      "  Epoch [4/30] Batch [2000/9467] Train Loss: 0.0323\n",
      "  Epoch [4/30] Batch [2050/9467] Train Loss: 0.0262\n",
      "  Epoch [4/30] Batch [2100/9467] Train Loss: 0.0391\n",
      "  Epoch [4/30] Batch [2150/9467] Train Loss: 0.0177\n",
      "  Epoch [4/30] Batch [2200/9467] Train Loss: 0.0418\n",
      "  Epoch [4/30] Batch [2250/9467] Train Loss: 0.0300\n",
      "  Epoch [4/30] Batch [2300/9467] Train Loss: 0.0452\n",
      "  Epoch [4/30] Batch [2350/9467] Train Loss: 0.0322\n",
      "  Epoch [4/30] Batch [2400/9467] Train Loss: 0.0606\n",
      "  Epoch [4/30] Batch [2450/9467] Train Loss: 0.0314\n",
      "  Epoch [4/30] Batch [2500/9467] Train Loss: 0.0251\n",
      "  Epoch [4/30] Batch [2550/9467] Train Loss: 0.0250\n",
      "  Epoch [4/30] Batch [2600/9467] Train Loss: 0.0315\n",
      "  Epoch [4/30] Batch [2650/9467] Train Loss: 0.0821\n",
      "  Epoch [4/30] Batch [2700/9467] Train Loss: 0.1047\n",
      "  Epoch [4/30] Batch [2750/9467] Train Loss: 0.0273\n",
      "  Epoch [4/30] Batch [2800/9467] Train Loss: 0.0464\n",
      "  Epoch [4/30] Batch [2850/9467] Train Loss: 0.0288\n",
      "  Epoch [4/30] Batch [2900/9467] Train Loss: 0.0180\n",
      "  Epoch [4/30] Batch [2950/9467] Train Loss: 0.0412\n",
      "  Epoch [4/30] Batch [3000/9467] Train Loss: 0.0294\n",
      "  Epoch [4/30] Batch [3050/9467] Train Loss: 0.1070\n",
      "  Epoch [4/30] Batch [3100/9467] Train Loss: 0.0357\n",
      "  Epoch [4/30] Batch [3150/9467] Train Loss: 0.0177\n",
      "  Epoch [4/30] Batch [3200/9467] Train Loss: 0.0562\n",
      "  Epoch [4/30] Batch [3250/9467] Train Loss: 0.0741\n",
      "  Epoch [4/30] Batch [3300/9467] Train Loss: 0.0234\n",
      "  Epoch [4/30] Batch [3350/9467] Train Loss: 0.0111\n",
      "  Epoch [4/30] Batch [3400/9467] Train Loss: 0.0322\n",
      "  Epoch [4/30] Batch [3450/9467] Train Loss: 0.0255\n",
      "  Epoch [4/30] Batch [3500/9467] Train Loss: 0.0132\n",
      "  Epoch [4/30] Batch [3550/9467] Train Loss: 0.0238\n",
      "  Epoch [4/30] Batch [3600/9467] Train Loss: 0.0657\n",
      "  Epoch [4/30] Batch [3650/9467] Train Loss: 0.0282\n",
      "  Epoch [4/30] Batch [3700/9467] Train Loss: 0.0393\n",
      "  Epoch [4/30] Batch [3750/9467] Train Loss: 0.0404\n",
      "  Epoch [4/30] Batch [3800/9467] Train Loss: 0.0990\n",
      "  Epoch [4/30] Batch [3850/9467] Train Loss: 0.0443\n",
      "  Epoch [4/30] Batch [3900/9467] Train Loss: 0.0445\n",
      "  Epoch [4/30] Batch [3950/9467] Train Loss: 0.0147\n",
      "  Epoch [4/30] Batch [4000/9467] Train Loss: 0.1001\n",
      "  Epoch [4/30] Batch [4050/9467] Train Loss: 0.0336\n",
      "  Epoch [4/30] Batch [4100/9467] Train Loss: 0.0817\n",
      "  Epoch [4/30] Batch [4150/9467] Train Loss: 0.0306\n",
      "  Epoch [4/30] Batch [4200/9467] Train Loss: 0.0583\n",
      "  Epoch [4/30] Batch [4250/9467] Train Loss: 0.0312\n",
      "  Epoch [4/30] Batch [4300/9467] Train Loss: 0.0482\n",
      "  Epoch [4/30] Batch [4350/9467] Train Loss: 0.0372\n",
      "  Epoch [4/30] Batch [4400/9467] Train Loss: 0.0354\n",
      "  Epoch [4/30] Batch [4450/9467] Train Loss: 0.0426\n",
      "  Epoch [4/30] Batch [4500/9467] Train Loss: 0.0484\n",
      "  Epoch [4/30] Batch [4550/9467] Train Loss: 0.0527\n",
      "  Epoch [4/30] Batch [4600/9467] Train Loss: 0.0313\n",
      "  Epoch [4/30] Batch [4650/9467] Train Loss: 0.0092\n",
      "  Epoch [4/30] Batch [4700/9467] Train Loss: 0.0429\n",
      "  Epoch [4/30] Batch [4750/9467] Train Loss: 0.0255\n",
      "  Epoch [4/30] Batch [4800/9467] Train Loss: 0.0253\n",
      "  Epoch [4/30] Batch [4850/9467] Train Loss: 0.0091\n",
      "  Epoch [4/30] Batch [4900/9467] Train Loss: 0.0381\n",
      "  Epoch [4/30] Batch [4950/9467] Train Loss: 0.0311\n",
      "  Epoch [4/30] Batch [5000/9467] Train Loss: 0.0575\n",
      "  Epoch [4/30] Batch [5050/9467] Train Loss: 0.0468\n",
      "  Epoch [4/30] Batch [5100/9467] Train Loss: 0.0304\n",
      "  Epoch [4/30] Batch [5150/9467] Train Loss: 0.0239\n",
      "  Epoch [4/30] Batch [5200/9467] Train Loss: 0.0216\n",
      "  Epoch [4/30] Batch [5250/9467] Train Loss: 0.0172\n",
      "  Epoch [4/30] Batch [5300/9467] Train Loss: 0.0223\n",
      "  Epoch [4/30] Batch [5350/9467] Train Loss: 0.0499\n",
      "  Epoch [4/30] Batch [5400/9467] Train Loss: 0.0308\n",
      "  Epoch [4/30] Batch [5450/9467] Train Loss: 0.1037\n",
      "  Epoch [4/30] Batch [5500/9467] Train Loss: 0.0191\n",
      "  Epoch [4/30] Batch [5550/9467] Train Loss: 0.0196\n",
      "  Epoch [4/30] Batch [5600/9467] Train Loss: 0.0424\n",
      "  Epoch [4/30] Batch [5650/9467] Train Loss: 0.0456\n",
      "  Epoch [4/30] Batch [5700/9467] Train Loss: 0.0387\n",
      "  Epoch [4/30] Batch [5750/9467] Train Loss: 0.0351\n",
      "  Epoch [4/30] Batch [5800/9467] Train Loss: 0.0135\n",
      "  Epoch [4/30] Batch [5850/9467] Train Loss: 0.0406\n",
      "  Epoch [4/30] Batch [5900/9467] Train Loss: 0.0237\n",
      "  Epoch [4/30] Batch [5950/9467] Train Loss: 0.0746\n",
      "  Epoch [4/30] Batch [6000/9467] Train Loss: 0.1385\n",
      "  Epoch [4/30] Batch [6050/9467] Train Loss: 0.0759\n",
      "  Epoch [4/30] Batch [6100/9467] Train Loss: 0.0149\n",
      "  Epoch [4/30] Batch [6150/9467] Train Loss: 0.0354\n",
      "  Epoch [4/30] Batch [6200/9467] Train Loss: 0.0704\n",
      "  Epoch [4/30] Batch [6250/9467] Train Loss: 0.0263\n",
      "  Epoch [4/30] Batch [6300/9467] Train Loss: 0.0436\n",
      "  Epoch [4/30] Batch [6350/9467] Train Loss: 0.0560\n",
      "  Epoch [4/30] Batch [6400/9467] Train Loss: 0.1520\n",
      "  Epoch [4/30] Batch [6450/9467] Train Loss: 0.0789\n",
      "  Epoch [4/30] Batch [6500/9467] Train Loss: 0.0381\n",
      "  Epoch [4/30] Batch [6550/9467] Train Loss: 0.0136\n",
      "  Epoch [4/30] Batch [6600/9467] Train Loss: 0.0425\n",
      "  Epoch [4/30] Batch [6650/9467] Train Loss: 0.0401\n",
      "  Epoch [4/30] Batch [6700/9467] Train Loss: 0.2405\n",
      "  Epoch [4/30] Batch [6750/9467] Train Loss: 0.0167\n",
      "  Epoch [4/30] Batch [6800/9467] Train Loss: 0.0557\n",
      "  Epoch [4/30] Batch [6850/9467] Train Loss: 0.0856\n",
      "  Epoch [4/30] Batch [6900/9467] Train Loss: 0.0329\n",
      "  Epoch [4/30] Batch [6950/9467] Train Loss: 0.0381\n",
      "  Epoch [4/30] Batch [7000/9467] Train Loss: 0.0403\n",
      "  Epoch [4/30] Batch [7050/9467] Train Loss: 0.1116\n",
      "  Epoch [4/30] Batch [7100/9467] Train Loss: 0.0364\n",
      "  Epoch [4/30] Batch [7150/9467] Train Loss: 0.0047\n",
      "  Epoch [4/30] Batch [7200/9467] Train Loss: 0.0874\n",
      "  Epoch [4/30] Batch [7250/9467] Train Loss: 0.0352\n",
      "  Epoch [4/30] Batch [7300/9467] Train Loss: 0.0182\n",
      "  Epoch [4/30] Batch [7350/9467] Train Loss: 0.0490\n",
      "  Epoch [4/30] Batch [7400/9467] Train Loss: 0.0332\n",
      "  Epoch [4/30] Batch [7450/9467] Train Loss: 0.0219\n",
      "  Epoch [4/30] Batch [7500/9467] Train Loss: 0.0630\n",
      "  Epoch [4/30] Batch [7550/9467] Train Loss: 0.0620\n",
      "  Epoch [4/30] Batch [7600/9467] Train Loss: 0.0531\n",
      "  Epoch [4/30] Batch [7650/9467] Train Loss: 0.0681\n",
      "  Epoch [4/30] Batch [7700/9467] Train Loss: 0.0601\n",
      "  Epoch [4/30] Batch [7750/9467] Train Loss: 0.0164\n",
      "  Epoch [4/30] Batch [7800/9467] Train Loss: 0.0356\n",
      "  Epoch [4/30] Batch [7850/9467] Train Loss: 0.0243\n",
      "  Epoch [4/30] Batch [7900/9467] Train Loss: 0.1142\n",
      "  Epoch [4/30] Batch [7950/9467] Train Loss: 0.0308\n",
      "  Epoch [4/30] Batch [8000/9467] Train Loss: 0.2890\n",
      "  Epoch [4/30] Batch [8050/9467] Train Loss: 0.3155\n",
      "  Epoch [4/30] Batch [8100/9467] Train Loss: 0.0184\n",
      "  Epoch [4/30] Batch [8150/9467] Train Loss: 0.1190\n",
      "  Epoch [4/30] Batch [8200/9467] Train Loss: 0.0359\n",
      "  Epoch [4/30] Batch [8250/9467] Train Loss: 0.1126\n",
      "  Epoch [4/30] Batch [8300/9467] Train Loss: 0.0251\n",
      "  Epoch [4/30] Batch [8350/9467] Train Loss: 0.0410\n",
      "  Epoch [4/30] Batch [8400/9467] Train Loss: 0.0168\n",
      "  Epoch [4/30] Batch [8450/9467] Train Loss: 0.1179\n",
      "  Epoch [4/30] Batch [8500/9467] Train Loss: 0.0785\n",
      "  Epoch [4/30] Batch [8550/9467] Train Loss: 0.0296\n",
      "  Epoch [4/30] Batch [8600/9467] Train Loss: 0.0134\n",
      "  Epoch [4/30] Batch [8650/9467] Train Loss: 0.0706\n",
      "  Epoch [4/30] Batch [8700/9467] Train Loss: 0.0284\n",
      "  Epoch [4/30] Batch [8750/9467] Train Loss: 0.0404\n",
      "  Epoch [4/30] Batch [8800/9467] Train Loss: 0.0836\n",
      "  Epoch [4/30] Batch [8850/9467] Train Loss: 0.0299\n",
      "  Epoch [4/30] Batch [8900/9467] Train Loss: 0.3344\n",
      "  Epoch [4/30] Batch [8950/9467] Train Loss: 0.0434\n",
      "  Epoch [4/30] Batch [9000/9467] Train Loss: 0.0205\n",
      "  Epoch [4/30] Batch [9050/9467] Train Loss: 0.0598\n",
      "  Epoch [4/30] Batch [9100/9467] Train Loss: 0.0564\n",
      "  Epoch [4/30] Batch [9150/9467] Train Loss: 0.0270\n",
      "  Epoch [4/30] Batch [9200/9467] Train Loss: 0.0698\n",
      "  Epoch [4/30] Batch [9250/9467] Train Loss: 0.0305\n",
      "  Epoch [4/30] Batch [9300/9467] Train Loss: 0.0797\n",
      "  Epoch [4/30] Batch [9350/9467] Train Loss: 0.0270\n",
      "  Epoch [4/30] Batch [9400/9467] Train Loss: 0.1185\n",
      "  Epoch [4/30] Batch [9450/9467] Train Loss: 0.0483\n",
      "Epoch [4/30] Train Loss: 0.0541, AUC: 0.9948, PrAUC : 0.1975, BalAcc: 0.9444, F1: 0.9469, Recall: 0.9915, Precision: 0.9062, Time: 1114.86s\n",
      "Epoch [4/30] Val Loss: 0.2309, AUC: 0.8533, PrAUC: 0.151323, BalAcc: 0.7664, F1(pos): 0.0140, Recall(pos): 0.6170, Precision(pos): 0.0071, Time: 173.94s\n",
      "Matriz de confusion para epoca 4:\n",
      "[[44028  4053]\n",
      " [   18    29]]\n",
      "Val AUC (0.1513) no mejoro desde la mejor (0.1558). Ninguna mejora durante 3 epochs.\n",
      "Duracion epoca 4: 1288.82s\n",
      "\n",
      "===== Epoch 5/30 =====\n",
      "Learning rate: 1.000000e-04\n",
      "  Epoch [5/30] Batch [50/9467] Train Loss: 0.0659\n",
      "  Epoch [5/30] Batch [100/9467] Train Loss: 0.0218\n",
      "  Epoch [5/30] Batch [150/9467] Train Loss: 0.1076\n",
      "  Epoch [5/30] Batch [200/9467] Train Loss: 0.0460\n",
      "  Epoch [5/30] Batch [250/9467] Train Loss: 0.0399\n",
      "  Epoch [5/30] Batch [300/9467] Train Loss: 0.0600\n",
      "  Epoch [5/30] Batch [350/9467] Train Loss: 0.0537\n",
      "  Epoch [5/30] Batch [400/9467] Train Loss: 0.1765\n",
      "  Epoch [5/30] Batch [450/9467] Train Loss: 0.0678\n",
      "  Epoch [5/30] Batch [500/9467] Train Loss: 0.0353\n",
      "  Epoch [5/30] Batch [550/9467] Train Loss: 0.0282\n",
      "  Epoch [5/30] Batch [600/9467] Train Loss: 0.0327\n",
      "  Epoch [5/30] Batch [650/9467] Train Loss: 0.0413\n",
      "  Epoch [5/30] Batch [700/9467] Train Loss: 0.0499\n",
      "  Epoch [5/30] Batch [750/9467] Train Loss: 0.1315\n",
      "  Epoch [5/30] Batch [800/9467] Train Loss: 0.0247\n",
      "  Epoch [5/30] Batch [850/9467] Train Loss: 0.1150\n",
      "  Epoch [5/30] Batch [900/9467] Train Loss: 0.0347\n",
      "  Epoch [5/30] Batch [950/9467] Train Loss: 0.0400\n",
      "  Epoch [5/30] Batch [1000/9467] Train Loss: 0.0145\n",
      "  Epoch [5/30] Batch [1050/9467] Train Loss: 0.0452\n",
      "  Epoch [5/30] Batch [1100/9467] Train Loss: 0.0393\n",
      "  Epoch [5/30] Batch [1150/9467] Train Loss: 0.0425\n",
      "  Epoch [5/30] Batch [1200/9467] Train Loss: 0.0213\n",
      "  Epoch [5/30] Batch [1250/9467] Train Loss: 0.0311\n",
      "  Epoch [5/30] Batch [1300/9467] Train Loss: 0.1186\n",
      "  Epoch [5/30] Batch [1350/9467] Train Loss: 0.0673\n",
      "  Epoch [5/30] Batch [1400/9467] Train Loss: 0.0135\n",
      "  Epoch [5/30] Batch [1450/9467] Train Loss: 0.0391\n",
      "  Epoch [5/30] Batch [1500/9467] Train Loss: 0.0366\n",
      "  Epoch [5/30] Batch [1550/9467] Train Loss: 0.0368\n",
      "  Epoch [5/30] Batch [1600/9467] Train Loss: 0.0672\n",
      "  Epoch [5/30] Batch [1650/9467] Train Loss: 0.0382\n",
      "  Epoch [5/30] Batch [1700/9467] Train Loss: 0.0322\n",
      "  Epoch [5/30] Batch [1750/9467] Train Loss: 0.0395\n",
      "  Epoch [5/30] Batch [1800/9467] Train Loss: 0.0224\n",
      "  Epoch [5/30] Batch [1850/9467] Train Loss: 0.0574\n",
      "  Epoch [5/30] Batch [1900/9467] Train Loss: 0.2708\n",
      "  Epoch [5/30] Batch [1950/9467] Train Loss: 0.0473\n",
      "  Epoch [5/30] Batch [2000/9467] Train Loss: 0.1265\n",
      "  Epoch [5/30] Batch [2050/9467] Train Loss: 0.2163\n",
      "  Epoch [5/30] Batch [2100/9467] Train Loss: 0.0355\n",
      "  Epoch [5/30] Batch [2150/9467] Train Loss: 0.0256\n",
      "  Epoch [5/30] Batch [2200/9467] Train Loss: 0.1122\n",
      "  Epoch [5/30] Batch [2250/9467] Train Loss: 0.0337\n",
      "  Epoch [5/30] Batch [2300/9467] Train Loss: 0.0595\n",
      "  Epoch [5/30] Batch [2350/9467] Train Loss: 0.0353\n",
      "  Epoch [5/30] Batch [2400/9467] Train Loss: 0.0279\n",
      "  Epoch [5/30] Batch [2450/9467] Train Loss: 0.0342\n",
      "  Epoch [5/30] Batch [2500/9467] Train Loss: 0.0461\n",
      "  Epoch [5/30] Batch [2550/9467] Train Loss: 0.0178\n",
      "  Epoch [5/30] Batch [2600/9467] Train Loss: 0.0379\n",
      "  Epoch [5/30] Batch [2650/9467] Train Loss: 0.1088\n",
      "  Epoch [5/30] Batch [2700/9467] Train Loss: 0.1266\n",
      "  Epoch [5/30] Batch [2750/9467] Train Loss: 0.0628\n",
      "  Epoch [5/30] Batch [2800/9467] Train Loss: 0.0890\n",
      "  Epoch [5/30] Batch [2850/9467] Train Loss: 0.0263\n",
      "  Epoch [5/30] Batch [2900/9467] Train Loss: 0.0568\n",
      "  Epoch [5/30] Batch [2950/9467] Train Loss: 0.1083\n",
      "  Epoch [5/30] Batch [3000/9467] Train Loss: 0.0941\n",
      "  Epoch [5/30] Batch [3050/9467] Train Loss: 0.0432\n",
      "  Epoch [5/30] Batch [3100/9467] Train Loss: 0.0571\n",
      "  Epoch [5/30] Batch [3150/9467] Train Loss: 0.0653\n",
      "  Epoch [5/30] Batch [3200/9467] Train Loss: 0.0343\n",
      "  Epoch [5/30] Batch [3250/9467] Train Loss: 0.0558\n",
      "  Epoch [5/30] Batch [3300/9467] Train Loss: 0.0146\n",
      "  Epoch [5/30] Batch [3350/9467] Train Loss: 0.1214\n",
      "  Epoch [5/30] Batch [3400/9467] Train Loss: 0.0274\n",
      "  Epoch [5/30] Batch [3450/9467] Train Loss: 0.0570\n",
      "  Epoch [5/30] Batch [3500/9467] Train Loss: 0.0389\n",
      "  Epoch [5/30] Batch [3550/9467] Train Loss: 0.0972\n",
      "  Epoch [5/30] Batch [3600/9467] Train Loss: 0.0279\n",
      "  Epoch [5/30] Batch [3650/9467] Train Loss: 0.0400\n",
      "  Epoch [5/30] Batch [3700/9467] Train Loss: 0.0640\n",
      "  Epoch [5/30] Batch [3750/9467] Train Loss: 0.0465\n",
      "  Epoch [5/30] Batch [3800/9467] Train Loss: 0.0660\n",
      "  Epoch [5/30] Batch [3850/9467] Train Loss: 0.0364\n",
      "  Epoch [5/30] Batch [3900/9467] Train Loss: 0.0280\n",
      "  Epoch [5/30] Batch [3950/9467] Train Loss: 0.0142\n",
      "  Epoch [5/30] Batch [4000/9467] Train Loss: 0.0312\n",
      "  Epoch [5/30] Batch [4050/9467] Train Loss: 0.0286\n",
      "  Epoch [5/30] Batch [4100/9467] Train Loss: 0.0228\n",
      "  Epoch [5/30] Batch [4150/9467] Train Loss: 0.0549\n",
      "  Epoch [5/30] Batch [4200/9467] Train Loss: 0.0053\n",
      "  Epoch [5/30] Batch [4250/9467] Train Loss: 0.0371\n",
      "  Epoch [5/30] Batch [4300/9467] Train Loss: 0.0216\n",
      "  Epoch [5/30] Batch [4350/9467] Train Loss: 0.0379\n",
      "  Epoch [5/30] Batch [4400/9467] Train Loss: 0.0250\n",
      "  Epoch [5/30] Batch [4450/9467] Train Loss: 0.0606\n",
      "  Epoch [5/30] Batch [4500/9467] Train Loss: 0.0309\n",
      "  Epoch [5/30] Batch [4550/9467] Train Loss: 0.0146\n",
      "  Epoch [5/30] Batch [4600/9467] Train Loss: 0.0391\n",
      "  Epoch [5/30] Batch [4650/9467] Train Loss: 0.0236\n",
      "  Epoch [5/30] Batch [4700/9467] Train Loss: 0.0443\n",
      "  Epoch [5/30] Batch [4750/9467] Train Loss: 0.0579\n",
      "  Epoch [5/30] Batch [4800/9467] Train Loss: 0.0465\n",
      "  Epoch [5/30] Batch [4850/9467] Train Loss: 0.0581\n",
      "  Epoch [5/30] Batch [4900/9467] Train Loss: 0.0367\n",
      "  Epoch [5/30] Batch [4950/9467] Train Loss: 0.0193\n",
      "  Epoch [5/30] Batch [5000/9467] Train Loss: 0.0134\n",
      "  Epoch [5/30] Batch [5050/9467] Train Loss: 0.0292\n",
      "  Epoch [5/30] Batch [5100/9467] Train Loss: 0.1171\n",
      "  Epoch [5/30] Batch [5150/9467] Train Loss: 0.0456\n",
      "  Epoch [5/30] Batch [5200/9467] Train Loss: 0.0360\n",
      "  Epoch [5/30] Batch [5250/9467] Train Loss: 0.0853\n",
      "  Epoch [5/30] Batch [5300/9467] Train Loss: 0.0343\n",
      "  Epoch [5/30] Batch [5350/9467] Train Loss: 0.0388\n",
      "  Epoch [5/30] Batch [5400/9467] Train Loss: 0.0407\n",
      "  Epoch [5/30] Batch [5450/9467] Train Loss: 0.0109\n",
      "  Epoch [5/30] Batch [5500/9467] Train Loss: 0.0425\n",
      "  Epoch [5/30] Batch [5550/9467] Train Loss: 0.0766\n",
      "  Epoch [5/30] Batch [5600/9467] Train Loss: 0.0525\n",
      "  Epoch [5/30] Batch [5650/9467] Train Loss: 0.0456\n",
      "  Epoch [5/30] Batch [5700/9467] Train Loss: 0.3571\n",
      "  Epoch [5/30] Batch [5750/9467] Train Loss: 0.0427\n",
      "  Epoch [5/30] Batch [5800/9467] Train Loss: 0.0275\n",
      "  Epoch [5/30] Batch [5850/9467] Train Loss: 0.0371\n",
      "  Epoch [5/30] Batch [5900/9467] Train Loss: 0.0653\n",
      "  Epoch [5/30] Batch [5950/9467] Train Loss: 0.0585\n",
      "  Epoch [5/30] Batch [6000/9467] Train Loss: 0.0753\n",
      "  Epoch [5/30] Batch [6050/9467] Train Loss: 0.0264\n",
      "  Epoch [5/30] Batch [6100/9467] Train Loss: 0.2868\n",
      "  Epoch [5/30] Batch [6150/9467] Train Loss: 0.0105\n",
      "  Epoch [5/30] Batch [6200/9467] Train Loss: 0.0173\n",
      "  Epoch [5/30] Batch [6250/9467] Train Loss: 0.0520\n",
      "  Epoch [5/30] Batch [6300/9467] Train Loss: 0.0752\n",
      "  Epoch [5/30] Batch [6350/9467] Train Loss: 0.0570\n",
      "  Epoch [5/30] Batch [6400/9467] Train Loss: 0.0264\n",
      "  Epoch [5/30] Batch [6450/9467] Train Loss: 0.0581\n",
      "  Epoch [5/30] Batch [6500/9467] Train Loss: 0.0170\n",
      "  Epoch [5/30] Batch [6550/9467] Train Loss: 0.0461\n",
      "  Epoch [5/30] Batch [6600/9467] Train Loss: 0.0681\n",
      "  Epoch [5/30] Batch [6650/9467] Train Loss: 0.0337\n",
      "  Epoch [5/30] Batch [6700/9467] Train Loss: 0.0469\n",
      "  Epoch [5/30] Batch [6750/9467] Train Loss: 0.0811\n",
      "  Epoch [5/30] Batch [6800/9467] Train Loss: 0.0794\n",
      "  Epoch [5/30] Batch [6850/9467] Train Loss: 0.0665\n",
      "  Epoch [5/30] Batch [6900/9467] Train Loss: 0.0324\n",
      "  Epoch [5/30] Batch [6950/9467] Train Loss: 0.0187\n",
      "  Epoch [5/30] Batch [7000/9467] Train Loss: 0.0304\n",
      "  Epoch [5/30] Batch [7050/9467] Train Loss: 0.0129\n",
      "  Epoch [5/30] Batch [7100/9467] Train Loss: 0.1233\n",
      "  Epoch [5/30] Batch [7150/9467] Train Loss: 0.0221\n",
      "  Epoch [5/30] Batch [7200/9467] Train Loss: 0.0169\n",
      "  Epoch [5/30] Batch [7250/9467] Train Loss: 0.0616\n",
      "  Epoch [5/30] Batch [7300/9467] Train Loss: 0.1065\n",
      "  Epoch [5/30] Batch [7350/9467] Train Loss: 0.1352\n",
      "  Epoch [5/30] Batch [7400/9467] Train Loss: 0.0315\n",
      "  Epoch [5/30] Batch [7450/9467] Train Loss: 0.0095\n",
      "  Epoch [5/30] Batch [7500/9467] Train Loss: 0.0314\n",
      "  Epoch [5/30] Batch [7550/9467] Train Loss: 0.0611\n",
      "  Epoch [5/30] Batch [7600/9467] Train Loss: 0.0380\n",
      "  Epoch [5/30] Batch [7650/9467] Train Loss: 0.0256\n",
      "  Epoch [5/30] Batch [7700/9467] Train Loss: 0.0652\n",
      "  Epoch [5/30] Batch [7750/9467] Train Loss: 0.0383\n",
      "  Epoch [5/30] Batch [7800/9467] Train Loss: 0.0367\n",
      "  Epoch [5/30] Batch [7850/9467] Train Loss: 0.1154\n",
      "  Epoch [5/30] Batch [7900/9467] Train Loss: 0.0352\n",
      "  Epoch [5/30] Batch [7950/9467] Train Loss: 0.0455\n",
      "  Epoch [5/30] Batch [8000/9467] Train Loss: 0.0221\n",
      "  Epoch [5/30] Batch [8050/9467] Train Loss: 0.0491\n",
      "  Epoch [5/30] Batch [8100/9467] Train Loss: 0.0521\n",
      "  Epoch [5/30] Batch [8150/9467] Train Loss: 0.0515\n",
      "  Epoch [5/30] Batch [8200/9467] Train Loss: 0.0207\n",
      "  Epoch [5/30] Batch [8250/9467] Train Loss: 0.0570\n",
      "  Epoch [5/30] Batch [8300/9467] Train Loss: 0.0293\n",
      "  Epoch [5/30] Batch [8350/9467] Train Loss: 0.0094\n",
      "  Epoch [5/30] Batch [8400/9467] Train Loss: 0.0554\n",
      "  Epoch [5/30] Batch [8450/9467] Train Loss: 0.0291\n",
      "  Epoch [5/30] Batch [8500/9467] Train Loss: 0.0327\n",
      "  Epoch [5/30] Batch [8550/9467] Train Loss: 0.1020\n",
      "  Epoch [5/30] Batch [8600/9467] Train Loss: 0.0150\n",
      "  Epoch [5/30] Batch [8650/9467] Train Loss: 0.0166\n",
      "  Epoch [5/30] Batch [8700/9467] Train Loss: 0.0736\n",
      "  Epoch [5/30] Batch [8750/9467] Train Loss: 0.0984\n",
      "  Epoch [5/30] Batch [8800/9467] Train Loss: 0.0121\n",
      "  Epoch [5/30] Batch [8850/9467] Train Loss: 0.0471\n",
      "  Epoch [5/30] Batch [8900/9467] Train Loss: 0.0334\n",
      "  Epoch [5/30] Batch [8950/9467] Train Loss: 0.0286\n",
      "  Epoch [5/30] Batch [9000/9467] Train Loss: 0.0137\n",
      "  Epoch [5/30] Batch [9050/9467] Train Loss: 0.0299\n",
      "  Epoch [5/30] Batch [9100/9467] Train Loss: 0.0261\n",
      "  Epoch [5/30] Batch [9150/9467] Train Loss: 0.0281\n",
      "  Epoch [5/30] Batch [9200/9467] Train Loss: 0.0388\n",
      "  Epoch [5/30] Batch [9250/9467] Train Loss: 0.2084\n",
      "  Epoch [5/30] Batch [9300/9467] Train Loss: 0.0341\n",
      "  Epoch [5/30] Batch [9350/9467] Train Loss: 0.0134\n",
      "  Epoch [5/30] Batch [9400/9467] Train Loss: 0.0117\n",
      "  Epoch [5/30] Batch [9450/9467] Train Loss: 0.0128\n",
      "Epoch [5/30] Train Loss: 0.0523, AUC: 0.9951, PrAUC : 0.1977, BalAcc: 0.9471, F1: 0.9491, Recall: 0.9913, Precision: 0.9104, Time: 1584.27s\n",
      "Epoch [5/30] Val Loss: 0.2820, AUC: 0.8509, PrAUC: 0.150219, BalAcc: 0.7548, F1(pos): 0.0111, Recall(pos): 0.6170, Precision(pos): 0.0056, Time: 182.11s\n",
      "Matriz de confusion para epoca 5:\n",
      "[[42912  5169]\n",
      " [   18    29]]\n",
      "Val AUC (0.1502) no mejoro desde la mejor (0.1558). Ninguna mejora durante 4 epochs.\n",
      "Duracion epoca 5: 1766.41s\n",
      "\n",
      "===== Epoch 6/30 =====\n",
      "Learning rate: 1.000000e-04\n",
      "  Epoch [6/30] Batch [50/9467] Train Loss: 0.0464\n",
      "  Epoch [6/30] Batch [100/9467] Train Loss: 0.0148\n",
      "  Epoch [6/30] Batch [150/9467] Train Loss: 0.0266\n",
      "  Epoch [6/30] Batch [200/9467] Train Loss: 0.0582\n",
      "  Epoch [6/30] Batch [250/9467] Train Loss: 0.0696\n",
      "  Epoch [6/30] Batch [300/9467] Train Loss: 0.0236\n",
      "  Epoch [6/30] Batch [350/9467] Train Loss: 0.0285\n",
      "  Epoch [6/30] Batch [400/9467] Train Loss: 0.0145\n",
      "  Epoch [6/30] Batch [450/9467] Train Loss: 0.0274\n",
      "  Epoch [6/30] Batch [500/9467] Train Loss: 0.1017\n",
      "  Epoch [6/30] Batch [550/9467] Train Loss: 0.0133\n",
      "  Epoch [6/30] Batch [600/9467] Train Loss: 0.1078\n",
      "  Epoch [6/30] Batch [650/9467] Train Loss: 0.0433\n",
      "  Epoch [6/30] Batch [700/9467] Train Loss: 0.0321\n",
      "  Epoch [6/30] Batch [750/9467] Train Loss: 0.0156\n",
      "  Epoch [6/30] Batch [800/9467] Train Loss: 0.0201\n",
      "  Epoch [6/30] Batch [850/9467] Train Loss: 0.0570\n",
      "  Epoch [6/30] Batch [900/9467] Train Loss: 0.0247\n",
      "  Epoch [6/30] Batch [950/9467] Train Loss: 0.0139\n",
      "  Epoch [6/30] Batch [1000/9467] Train Loss: 0.0820\n",
      "  Epoch [6/30] Batch [1050/9467] Train Loss: 0.0337\n",
      "  Epoch [6/30] Batch [1100/9467] Train Loss: 0.0153\n",
      "  Epoch [6/30] Batch [1150/9467] Train Loss: 0.0099\n",
      "  Epoch [6/30] Batch [1200/9467] Train Loss: 0.0242\n",
      "  Epoch [6/30] Batch [1250/9467] Train Loss: 0.0163\n",
      "  Epoch [6/30] Batch [1300/9467] Train Loss: 0.0617\n",
      "  Epoch [6/30] Batch [1350/9467] Train Loss: 0.0334\n",
      "  Epoch [6/30] Batch [1400/9467] Train Loss: 0.0121\n",
      "  Epoch [6/30] Batch [1450/9467] Train Loss: 0.0827\n",
      "  Epoch [6/30] Batch [1500/9467] Train Loss: 0.0213\n",
      "  Epoch [6/30] Batch [1550/9467] Train Loss: 0.0306\n",
      "  Epoch [6/30] Batch [1600/9467] Train Loss: 0.0165\n",
      "  Epoch [6/30] Batch [1650/9467] Train Loss: 0.2723\n",
      "  Epoch [6/30] Batch [1700/9467] Train Loss: 0.0936\n",
      "  Epoch [6/30] Batch [1750/9467] Train Loss: 0.0082\n",
      "  Epoch [6/30] Batch [1800/9467] Train Loss: 0.0717\n",
      "  Epoch [6/30] Batch [1850/9467] Train Loss: 0.0145\n",
      "  Epoch [6/30] Batch [1900/9467] Train Loss: 0.0194\n",
      "  Epoch [6/30] Batch [1950/9467] Train Loss: 0.0501\n",
      "  Epoch [6/30] Batch [2000/9467] Train Loss: 0.0360\n",
      "  Epoch [6/30] Batch [2050/9467] Train Loss: 0.0772\n",
      "  Epoch [6/30] Batch [2100/9467] Train Loss: 0.0622\n",
      "  Epoch [6/30] Batch [2150/9467] Train Loss: 0.0246\n",
      "  Epoch [6/30] Batch [2200/9467] Train Loss: 0.0550\n",
      "  Epoch [6/30] Batch [2250/9467] Train Loss: 0.0353\n",
      "  Epoch [6/30] Batch [2300/9467] Train Loss: 0.0836\n",
      "  Epoch [6/30] Batch [2350/9467] Train Loss: 0.0520\n",
      "  Epoch [6/30] Batch [2400/9467] Train Loss: 0.0185\n",
      "  Epoch [6/30] Batch [2450/9467] Train Loss: 0.0884\n",
      "  Epoch [6/30] Batch [2500/9467] Train Loss: 0.0533\n",
      "  Epoch [6/30] Batch [2550/9467] Train Loss: 0.0318\n",
      "  Epoch [6/30] Batch [2600/9467] Train Loss: 0.0423\n",
      "  Epoch [6/30] Batch [2650/9467] Train Loss: 0.0922\n",
      "  Epoch [6/30] Batch [2700/9467] Train Loss: 0.6648\n",
      "  Epoch [6/30] Batch [2750/9467] Train Loss: 0.0488\n",
      "  Epoch [6/30] Batch [2800/9467] Train Loss: 0.1163\n",
      "  Epoch [6/30] Batch [2850/9467] Train Loss: 0.0530\n",
      "  Epoch [6/30] Batch [2900/9467] Train Loss: 0.0625\n",
      "  Epoch [6/30] Batch [2950/9467] Train Loss: 0.3473\n",
      "  Epoch [6/30] Batch [3000/9467] Train Loss: 0.0385\n",
      "  Epoch [6/30] Batch [3050/9467] Train Loss: 0.0553\n",
      "  Epoch [6/30] Batch [3100/9467] Train Loss: 0.0507\n",
      "  Epoch [6/30] Batch [3150/9467] Train Loss: 0.0389\n",
      "  Epoch [6/30] Batch [3200/9467] Train Loss: 0.0383\n",
      "  Epoch [6/30] Batch [3250/9467] Train Loss: 0.0315\n",
      "  Epoch [6/30] Batch [3300/9467] Train Loss: 0.0486\n",
      "  Epoch [6/30] Batch [3350/9467] Train Loss: 0.0606\n",
      "  Epoch [6/30] Batch [3400/9467] Train Loss: 0.0419\n",
      "  Epoch [6/30] Batch [3450/9467] Train Loss: 0.0290\n",
      "  Epoch [6/30] Batch [3500/9467] Train Loss: 0.0605\n",
      "  Epoch [6/30] Batch [3550/9467] Train Loss: 0.0357\n",
      "  Epoch [6/30] Batch [3600/9467] Train Loss: 0.0466\n",
      "  Epoch [6/30] Batch [3650/9467] Train Loss: 0.0307\n",
      "  Epoch [6/30] Batch [3700/9467] Train Loss: 0.0340\n",
      "  Epoch [6/30] Batch [3750/9467] Train Loss: 0.0355\n",
      "  Epoch [6/30] Batch [3800/9467] Train Loss: 0.0951\n",
      "  Epoch [6/30] Batch [3850/9467] Train Loss: 0.0151\n",
      "  Epoch [6/30] Batch [3900/9467] Train Loss: 0.0214\n",
      "  Epoch [6/30] Batch [3950/9467] Train Loss: 0.0193\n",
      "  Epoch [6/30] Batch [4000/9467] Train Loss: 0.0213\n",
      "  Epoch [6/30] Batch [4050/9467] Train Loss: 0.0440\n",
      "  Epoch [6/30] Batch [4100/9467] Train Loss: 0.1241\n",
      "  Epoch [6/30] Batch [4150/9467] Train Loss: 0.0310\n",
      "  Epoch [6/30] Batch [4200/9467] Train Loss: 0.0678\n",
      "  Epoch [6/30] Batch [4250/9467] Train Loss: 0.0192\n",
      "  Epoch [6/30] Batch [4300/9467] Train Loss: 0.0612\n",
      "  Epoch [6/30] Batch [4350/9467] Train Loss: 0.0407\n",
      "  Epoch [6/30] Batch [4400/9467] Train Loss: 0.0639\n",
      "  Epoch [6/30] Batch [4450/9467] Train Loss: 0.0437\n",
      "  Epoch [6/30] Batch [4500/9467] Train Loss: 0.0400\n",
      "  Epoch [6/30] Batch [4550/9467] Train Loss: 0.0429\n",
      "  Epoch [6/30] Batch [4600/9467] Train Loss: 0.0167\n",
      "  Epoch [6/30] Batch [4650/9467] Train Loss: 0.0179\n",
      "  Epoch [6/30] Batch [4700/9467] Train Loss: 0.0228\n",
      "  Epoch [6/30] Batch [4750/9467] Train Loss: 0.0237\n",
      "  Epoch [6/30] Batch [4800/9467] Train Loss: 0.0395\n",
      "  Epoch [6/30] Batch [4850/9467] Train Loss: 0.0947\n",
      "  Epoch [6/30] Batch [4900/9467] Train Loss: 0.0164\n",
      "  Epoch [6/30] Batch [4950/9467] Train Loss: 0.0665\n",
      "  Epoch [6/30] Batch [5000/9467] Train Loss: 0.0743\n",
      "  Epoch [6/30] Batch [5050/9467] Train Loss: 0.0122\n",
      "  Epoch [6/30] Batch [5100/9467] Train Loss: 0.0325\n",
      "  Epoch [6/30] Batch [5150/9467] Train Loss: 0.0276\n",
      "  Epoch [6/30] Batch [5200/9467] Train Loss: 0.0409\n",
      "  Epoch [6/30] Batch [5250/9467] Train Loss: 0.0360\n",
      "  Epoch [6/30] Batch [5300/9467] Train Loss: 0.0360\n",
      "  Epoch [6/30] Batch [5350/9467] Train Loss: 0.0425\n",
      "  Epoch [6/30] Batch [5400/9467] Train Loss: 0.0569\n",
      "  Epoch [6/30] Batch [5450/9467] Train Loss: 0.0489\n",
      "  Epoch [6/30] Batch [5500/9467] Train Loss: 0.1319\n",
      "  Epoch [6/30] Batch [5550/9467] Train Loss: 0.0202\n",
      "  Epoch [6/30] Batch [5600/9467] Train Loss: 0.0228\n",
      "  Epoch [6/30] Batch [5650/9467] Train Loss: 0.0410\n",
      "  Epoch [6/30] Batch [5700/9467] Train Loss: 0.0143\n",
      "  Epoch [6/30] Batch [5750/9467] Train Loss: 0.0214\n",
      "  Epoch [6/30] Batch [5800/9467] Train Loss: 0.0113\n",
      "  Epoch [6/30] Batch [5850/9467] Train Loss: 0.1021\n",
      "  Epoch [6/30] Batch [5900/9467] Train Loss: 0.0330\n",
      "  Epoch [6/30] Batch [5950/9467] Train Loss: 0.0511\n",
      "  Epoch [6/30] Batch [6000/9467] Train Loss: 0.0369\n",
      "  Epoch [6/30] Batch [6050/9467] Train Loss: 0.0374\n",
      "  Epoch [6/30] Batch [6100/9467] Train Loss: 0.0323\n",
      "  Epoch [6/30] Batch [6150/9467] Train Loss: 0.0583\n",
      "  Epoch [6/30] Batch [6200/9467] Train Loss: 0.0224\n",
      "  Epoch [6/30] Batch [6250/9467] Train Loss: 0.0128\n",
      "  Epoch [6/30] Batch [6300/9467] Train Loss: 0.0200\n",
      "  Epoch [6/30] Batch [6350/9467] Train Loss: 0.1722\n",
      "  Epoch [6/30] Batch [6400/9467] Train Loss: 0.0450\n",
      "  Epoch [6/30] Batch [6450/9467] Train Loss: 0.0285\n",
      "  Epoch [6/30] Batch [6500/9467] Train Loss: 0.0249\n",
      "  Epoch [6/30] Batch [6550/9467] Train Loss: 0.1568\n",
      "  Epoch [6/30] Batch [6600/9467] Train Loss: 0.0094\n",
      "  Epoch [6/30] Batch [6650/9467] Train Loss: 0.0115\n",
      "  Epoch [6/30] Batch [6700/9467] Train Loss: 0.0314\n",
      "  Epoch [6/30] Batch [6750/9467] Train Loss: 0.1834\n",
      "  Epoch [6/30] Batch [6800/9467] Train Loss: 0.0207\n",
      "  Epoch [6/30] Batch [6850/9467] Train Loss: 0.0125\n",
      "  Epoch [6/30] Batch [6900/9467] Train Loss: 0.0855\n",
      "  Epoch [6/30] Batch [6950/9467] Train Loss: 0.0289\n",
      "  Epoch [6/30] Batch [7000/9467] Train Loss: 0.0347\n",
      "  Epoch [6/30] Batch [7050/9467] Train Loss: 0.0334\n",
      "  Epoch [6/30] Batch [7100/9467] Train Loss: 0.0574\n",
      "  Epoch [6/30] Batch [7150/9467] Train Loss: 0.0308\n",
      "  Epoch [6/30] Batch [7200/9467] Train Loss: 0.0294\n",
      "  Epoch [6/30] Batch [7250/9467] Train Loss: 0.0436\n",
      "  Epoch [6/30] Batch [7300/9467] Train Loss: 0.0830\n",
      "  Epoch [6/30] Batch [7350/9467] Train Loss: 0.0268\n",
      "  Epoch [6/30] Batch [7400/9467] Train Loss: 0.0210\n",
      "  Epoch [6/30] Batch [7450/9467] Train Loss: 0.0150\n",
      "  Epoch [6/30] Batch [7500/9467] Train Loss: 0.0241\n",
      "  Epoch [6/30] Batch [7550/9467] Train Loss: 0.1286\n",
      "  Epoch [6/30] Batch [7600/9467] Train Loss: 0.1019\n",
      "  Epoch [6/30] Batch [7650/9467] Train Loss: 0.0387\n",
      "  Epoch [6/30] Batch [7700/9467] Train Loss: 0.0456\n",
      "  Epoch [6/30] Batch [7750/9467] Train Loss: 0.0351\n",
      "  Epoch [6/30] Batch [7800/9467] Train Loss: 0.1097\n",
      "  Epoch [6/30] Batch [7850/9467] Train Loss: 0.0442\n",
      "  Epoch [6/30] Batch [7900/9467] Train Loss: 0.0844\n",
      "  Epoch [6/30] Batch [7950/9467] Train Loss: 0.0626\n",
      "  Epoch [6/30] Batch [8000/9467] Train Loss: 0.0220\n",
      "  Epoch [6/30] Batch [8050/9467] Train Loss: 0.0389\n",
      "  Epoch [6/30] Batch [8100/9467] Train Loss: 0.0823\n",
      "  Epoch [6/30] Batch [8150/9467] Train Loss: 0.1018\n",
      "  Epoch [6/30] Batch [8200/9467] Train Loss: 0.0314\n",
      "  Epoch [6/30] Batch [8250/9467] Train Loss: 0.0454\n",
      "  Epoch [6/30] Batch [8300/9467] Train Loss: 0.0488\n",
      "  Epoch [6/30] Batch [8350/9467] Train Loss: 0.0940\n",
      "  Epoch [6/30] Batch [8400/9467] Train Loss: 0.0260\n",
      "  Epoch [6/30] Batch [8450/9467] Train Loss: 0.0185\n",
      "  Epoch [6/30] Batch [8500/9467] Train Loss: 0.0377\n",
      "  Epoch [6/30] Batch [8550/9467] Train Loss: 0.0618\n",
      "  Epoch [6/30] Batch [8600/9467] Train Loss: 0.0429\n",
      "  Epoch [6/30] Batch [8650/9467] Train Loss: 0.0227\n",
      "  Epoch [6/30] Batch [8700/9467] Train Loss: 0.0348\n",
      "  Epoch [6/30] Batch [8750/9467] Train Loss: 0.0450\n",
      "  Epoch [6/30] Batch [8800/9467] Train Loss: 0.3050\n",
      "  Epoch [6/30] Batch [8850/9467] Train Loss: 0.2639\n",
      "  Epoch [6/30] Batch [8900/9467] Train Loss: 0.0583\n",
      "  Epoch [6/30] Batch [8950/9467] Train Loss: 0.0152\n",
      "  Epoch [6/30] Batch [9000/9467] Train Loss: 0.0454\n",
      "  Epoch [6/30] Batch [9050/9467] Train Loss: 0.0435\n",
      "  Epoch [6/30] Batch [9100/9467] Train Loss: 0.1215\n",
      "  Epoch [6/30] Batch [9150/9467] Train Loss: 0.0575\n",
      "  Epoch [6/30] Batch [9200/9467] Train Loss: 0.0908\n",
      "  Epoch [6/30] Batch [9250/9467] Train Loss: 0.0381\n",
      "  Epoch [6/30] Batch [9300/9467] Train Loss: 0.0410\n",
      "  Epoch [6/30] Batch [9350/9467] Train Loss: 0.0231\n",
      "  Epoch [6/30] Batch [9400/9467] Train Loss: 0.0454\n",
      "  Epoch [6/30] Batch [9450/9467] Train Loss: 0.0100\n",
      "Epoch [6/30] Train Loss: 0.0502, AUC: 0.9955, PrAUC : 0.1979, BalAcc: 0.9494, F1: 0.9513, Recall: 0.9919, Precision: 0.9139, Time: 1806.15s\n",
      "Epoch [6/30] Val Loss: 0.2465, AUC: 0.8479, PrAUC: 0.149229, BalAcc: 0.7631, F1(pos): 0.0131, Recall(pos): 0.6170, Precision(pos): 0.0066, Time: 169.24s\n",
      "Matriz de confusion para epoca 6:\n",
      "[[43719  4362]\n",
      " [   18    29]]\n",
      "Val AUC (0.1492) no mejoro desde la mejor (0.1558). Ninguna mejora durante 5 epochs.\n",
      "Early stopping 5 epochs sin mejora\n",
      "\n",
      " --- Entrenamiento finalizado ---\n",
      "Mejor Val PrAUC: 0.1558\n",
      "Mejor modelo guardado en: models/edgenext_last_layer/edgenext_best.pth\n",
      "Historial de entrenamiento en: models/edgenext_last_layer/training_history.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Empezando entrenamiento con NUM_WORKERS = {NUM_WORKERS}\")\n",
    "    start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76add97d-a5af-42fb-b003-cd6167c4559c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
